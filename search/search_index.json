{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Historiographing the Era of the American Revolution (HGEAR) The historical concept of democracy , liberty and freedom and its development through time is important to our understanding of the past. The goal of the HGEAR project is to chart shifts in the character and structure of political discourse during the era of the American Revolution. By comparing shifts in language usage (via ShiCo (Shifting Concepts Through Time ) with changing formations in correspondence networks, we are able to measure how networks of communication within the Founders Online corpus and other resources such as the Letters of Delegates to Congress shaped the character, distribution, and spread of political ideas during the Revolutionary era. This project and associated analyses offer insights into longstanding debates in the field of early American history. Also, by means of additional collected data, we show how the here included scripts and analyses can be easily accomodated to include other letter correspondence sources as well. Ultimately, this allows researchers to get a more complete picture of the fluxes in the political discourse during the American Revolution.","title":"Home"},{"location":"#historiographing-the-era-of-the-american-revolution-hgear","text":"The historical concept of democracy , liberty and freedom and its development through time is important to our understanding of the past. The goal of the HGEAR project is to chart shifts in the character and structure of political discourse during the era of the American Revolution. By comparing shifts in language usage (via ShiCo (Shifting Concepts Through Time ) with changing formations in correspondence networks, we are able to measure how networks of communication within the Founders Online corpus and other resources such as the Letters of Delegates to Congress shaped the character, distribution, and spread of political ideas during the Revolutionary era. This project and associated analyses offer insights into longstanding debates in the field of early American history. Also, by means of additional collected data, we show how the here included scripts and analyses can be easily accomodated to include other letter correspondence sources as well. Ultimately, this allows researchers to get a more complete picture of the fluxes in the political discourse during the American Revolution.","title":"Historiographing the Era of the American Revolution (HGEAR)"},{"location":"CODE_OF_CONDUCT/","text":"Contributor Covenant Code of Conduct Our Pledge We, as members, contributors, and leaders, pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We commit to acting and interacting in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community. Our Standards Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Using welcoming and inclusive language Showing empathy towards other community members Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct that could reasonably be considered inappropriate in a professional setting Enforcement Responsibilities Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior. They will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned with this Code of Conduct. They will communicate reasons for moderation decisions when appropriate. Scope This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official email address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Enforcement Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at [Email Address]. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident. Enforcement Guidelines Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: Correction Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. Warning Community Impact: A violation through a single incident or series of actions. Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. Temporary Ban Community Impact: A serious violation of community standards, including sustained inappropriate behavior. Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. Permanent Ban Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence: A permanent ban from any sort of public interaction within the community. Attribution This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at Contributor Covenant . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder. For answers to common questions about this code of conduct, see the FAQ . Translations are available at Translations .","title":"Code of Conduct"},{"location":"CODE_OF_CONDUCT/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"CODE_OF_CONDUCT/#our-pledge","text":"We, as members, contributors, and leaders, pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation. We commit to acting and interacting in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.","title":"Our Pledge"},{"location":"CODE_OF_CONDUCT/#our-standards","text":"Examples of behavior that contributes to a positive environment for our community include: Demonstrating empathy and kindness toward other people Being respectful of differing opinions, viewpoints, and experiences Giving and gracefully accepting constructive feedback Accepting responsibility and apologizing to those affected by our mistakes, and learning from the experience Focusing on what is best not just for us as individuals, but for the overall community Using welcoming and inclusive language Showing empathy towards other community members Examples of unacceptable behavior include: The use of sexualized language or imagery, and sexual attention or advances of any kind Trolling, insulting or derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or email address, without their explicit permission Other conduct that could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"CODE_OF_CONDUCT/#enforcement-responsibilities","text":"Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior. They will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful. Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned with this Code of Conduct. They will communicate reasons for moderation decisions when appropriate.","title":"Enforcement Responsibilities"},{"location":"CODE_OF_CONDUCT/#scope","text":"This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official email address, posting via an official social media account, or acting as an appointed representative at an online or offline event.","title":"Scope"},{"location":"CODE_OF_CONDUCT/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at [Email Address]. All complaints will be reviewed and investigated promptly and fairly. All community leaders are obligated to respect the privacy and security of the reporter of any incident.","title":"Enforcement"},{"location":"CODE_OF_CONDUCT/#enforcement-guidelines","text":"Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct: Correction Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community. Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested. Warning Community Impact: A violation through a single incident or series of actions. Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban. Temporary Ban Community Impact: A serious violation of community standards, including sustained inappropriate behavior. Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban. Permanent Ban Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals. Consequence: A permanent ban from any sort of public interaction within the community.","title":"Enforcement Guidelines"},{"location":"CODE_OF_CONDUCT/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at Contributor Covenant . Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder. For answers to common questions about this code of conduct, see the FAQ . Translations are available at Translations .","title":"Attribution"},{"location":"CONTRIBUTING/","text":"Contributing Guidelines Thank you for considering contributing to our project! We appreciate any form of contribution, from asking questions to submitting bug reports or proposing new features. Please review our Code of Conduct before engaging with the project. Types of Contributions 1. Asking Questions Use the search functionality to check for existing issues. If no relevant results are found, create a new issue. Apply the \"Question\" label and other relevant labels as needed. 2. Reporting Bugs Search for existing issues related to the bug. If not found, create a new issue with sufficient details (including commit SHA, dependency info, and OS details). Apply relevant labels to the newly created issue. 3. Making Code Changes (Important) Before making changes, announce your plan through a new issue. 1. Wait for community consensus on the proposed idea. 2. Fork the repository and create a feature branch. 3. Stay updated with the main branch by pulling changes. 4. Install dependencies (refer to development documentation ). 5. Add new tests if necessary and update/expand documentation. 6. Push your feature branch to your fork. 7. Create a pull request following these instructions . If you feel you've made a valuable contribution but need help with tests or documentation, submit the pull request, and we'll assist you. 4. Providing Experience, Feedback, or Suggestions Use the search function to check for similar experiences or suggestions. Open a new issue to share your experience, provide feedback, or suggest improvements/features. Getting started with development Setup for R Development To contribute to our project, you'll need to set up your development environment for R. Follow these steps: Install R and R Studio: Download and install R from CRAN . Download and install R Studio from here . Clone the Repository: Fork the repository to your GitHub account. Clone the forked repository to your local machine: bash git clone https://github.com/your-username/repository.git Create a Virtual Environment for R: R projects often use .Renviron or .Rprofile for custom environment settings. Check the project's documentation for any specific configurations. In R, the equivalent to Python's requirements.txt is typically a file named renv.lock. This file contains information about the dependencies required in a R project, including the versions of R and all installed packages. Set up a virtual environment using R's renv package. Navigate to the project directory and run: R install.packages(\"renv\") library(renv) renv::init() When you have cloned the project and run renv::init(), renv will use the information in the 'renv.lock' file to recreate the exact environment we used in the project. - Install project dependencies: R renv::install() Setup for Python Development To contribute to our project in Python, follow these steps: Install Python: Download and install Python from here . Clone the Repository: Fork the repository to your GitHub account. Clone the forked repository to your local machine: bash git clone https://github.com/your-username/repository.git Create a Virtual Environment for Python: Navigate to the project directory and create a virtual environment: bash python -m venv venv Activate the virtual environment: On Windows: venv\\Scripts\\activate On Unix or MacOS: source venv/bin/activate Install project dependencies: bash pip install -r requirements.txt","title":"Contributing guidelines"},{"location":"CONTRIBUTING/#contributing-guidelines","text":"Thank you for considering contributing to our project! We appreciate any form of contribution, from asking questions to submitting bug reports or proposing new features. Please review our Code of Conduct before engaging with the project.","title":"Contributing Guidelines"},{"location":"CONTRIBUTING/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"CONTRIBUTING/#1-asking-questions","text":"Use the search functionality to check for existing issues. If no relevant results are found, create a new issue. Apply the \"Question\" label and other relevant labels as needed.","title":"1. Asking Questions"},{"location":"CONTRIBUTING/#2-reporting-bugs","text":"Search for existing issues related to the bug. If not found, create a new issue with sufficient details (including commit SHA, dependency info, and OS details). Apply relevant labels to the newly created issue.","title":"2. Reporting Bugs"},{"location":"CONTRIBUTING/#3-making-code-changes","text":"(Important) Before making changes, announce your plan through a new issue. 1. Wait for community consensus on the proposed idea. 2. Fork the repository and create a feature branch. 3. Stay updated with the main branch by pulling changes. 4. Install dependencies (refer to development documentation ). 5. Add new tests if necessary and update/expand documentation. 6. Push your feature branch to your fork. 7. Create a pull request following these instructions . If you feel you've made a valuable contribution but need help with tests or documentation, submit the pull request, and we'll assist you.","title":"3. Making Code Changes"},{"location":"CONTRIBUTING/#4-providing-experience-feedback-or-suggestions","text":"Use the search function to check for similar experiences or suggestions. Open a new issue to share your experience, provide feedback, or suggest improvements/features.","title":"4. Providing Experience, Feedback, or Suggestions"},{"location":"CONTRIBUTING/#getting-started-with-development","text":"","title":"Getting started with development"},{"location":"CONTRIBUTING/#setup-for-r-development","text":"To contribute to our project, you'll need to set up your development environment for R. Follow these steps: Install R and R Studio: Download and install R from CRAN . Download and install R Studio from here . Clone the Repository: Fork the repository to your GitHub account. Clone the forked repository to your local machine: bash git clone https://github.com/your-username/repository.git Create a Virtual Environment for R: R projects often use .Renviron or .Rprofile for custom environment settings. Check the project's documentation for any specific configurations. In R, the equivalent to Python's requirements.txt is typically a file named renv.lock. This file contains information about the dependencies required in a R project, including the versions of R and all installed packages. Set up a virtual environment using R's renv package. Navigate to the project directory and run: R install.packages(\"renv\") library(renv) renv::init() When you have cloned the project and run renv::init(), renv will use the information in the 'renv.lock' file to recreate the exact environment we used in the project. - Install project dependencies: R renv::install()","title":"Setup for R Development"},{"location":"CONTRIBUTING/#setup-for-python-development","text":"To contribute to our project in Python, follow these steps: Install Python: Download and install Python from here . Clone the Repository: Fork the repository to your GitHub account. Clone the forked repository to your local machine: bash git clone https://github.com/your-username/repository.git Create a Virtual Environment for Python: Navigate to the project directory and create a virtual environment: bash python -m venv venv Activate the virtual environment: On Windows: venv\\Scripts\\activate On Unix or MacOS: source venv/bin/activate Install project dependencies: bash pip install -r requirements.txt","title":"Setup for Python Development"},{"location":"About/ACTION/","text":"H-gear in action This section includes links to projects, events, papers, blogposts, videos, etc. which are related to the H-GEAR project. Blogs Discoveries through time: how the H-GEAR project is connecting a network of the most influential people of the American Revolution","title":"In action"},{"location":"About/ACTION/#h-gear-in-action","text":"This section includes links to projects, events, papers, blogposts, videos, etc. which are related to the H-GEAR project. Blogs Discoveries through time: how the H-GEAR project is connecting a network of the most influential people of the American Revolution","title":"H-gear in action"},{"location":"About/CITE/","text":".mint-green-code-block { background-color: #98FF98; /* Mint green background */ color: black; /* Black text color */ padding: 10px; /* Padding around the text */ border-radius: 5px; /* Rounded corners */ } .mint-green-code-block b { font-weight: bold; /* Bold text */ } How to cite If you use this software in your work, please cite it using the following metadata (Learn more about CITATION files). You can do this easily with one of these options: The 'cite this repository' menu in the right tab at https://github.com/h-gear/revolution. The citation file . If you prefer to copypaste, here you have the APA and BibTex strings: APA : Vroegh, T.(2024). Revolution (Version 0.0.1) [Computer software]. https://github.com/h-gear/revolution BibTex : @software{revolution, author = {Vroegh, Thijs}, title = {revolution (Version 0.0.1) [Computer software]}, url = {https://github.com/h-gear/revolution}","title":"How to Cite"},{"location":"About/CITE/#how-to-cite","text":"If you use this software in your work, please cite it using the following metadata (Learn more about CITATION files). You can do this easily with one of these options: The 'cite this repository' menu in the right tab at https://github.com/h-gear/revolution. The citation file . If you prefer to copypaste, here you have the APA and BibTex strings: APA : Vroegh, T.(2024). Revolution (Version 0.0.1) [Computer software]. https://github.com/h-gear/revolution BibTex : @software{revolution, author = {Vroegh, Thijs}, title = {revolution (Version 0.0.1) [Computer software]}, url = {https://github.com/h-gear/revolution}","title":"How to cite"},{"location":"About/CONTACT/","text":"Contact The project on the American Revolution is maintained by the Netherlands eScience Center in collaboration with American Studies at the Research Centre for the Study of Democratic Cultures and Politics (DemCP) at the University of Groningen . If you have any questions, feedback, or need support, please feel free to reach out to us. Below are the primary contacts and useful links for your reference: General Inquiries For general questions and information, you can reach the Principal Investigator Dr. Mark L. Thompson: Email : m.l.thompson@rug.nl Phone : +31 50 36 35077 Website : https://www.rug.nl/staff/m.l.thompson/ Technical Support For technical support and troubleshooting, please contact our Lead Research Software Engineer Thijs Vroegh: Email : t.vroegh@esciencecenter.nl Documentation : https://revolution.readthedocs.io/en/latest/ Collaboration If you are interested in contributing, reach out at: Email : info@esciencecenter.nl Github : https://github.com/h-gear/revolution Follow Us Stay updated on the project by following us on: RSD : Research Software Directory LinkedIn : https://www.linkedin.com/company/netherlands-escience-center/ For more detailed information, visit our official website . Thank you for your interest and support. We look forward to assisting you!","title":"Contact information"},{"location":"About/CONTACT/#contact","text":"The project on the American Revolution is maintained by the Netherlands eScience Center in collaboration with American Studies at the Research Centre for the Study of Democratic Cultures and Politics (DemCP) at the University of Groningen . If you have any questions, feedback, or need support, please feel free to reach out to us. Below are the primary contacts and useful links for your reference: General Inquiries For general questions and information, you can reach the Principal Investigator Dr. Mark L. Thompson: Email : m.l.thompson@rug.nl Phone : +31 50 36 35077 Website : https://www.rug.nl/staff/m.l.thompson/ Technical Support For technical support and troubleshooting, please contact our Lead Research Software Engineer Thijs Vroegh: Email : t.vroegh@esciencecenter.nl Documentation : https://revolution.readthedocs.io/en/latest/ Collaboration If you are interested in contributing, reach out at: Email : info@esciencecenter.nl Github : https://github.com/h-gear/revolution Follow Us Stay updated on the project by following us on: RSD : Research Software Directory LinkedIn : https://www.linkedin.com/company/netherlands-escience-center/ For more detailed information, visit our official website . Thank you for your interest and support. We look forward to assisting you!","title":"Contact"},{"location":"Analyses/COLLECTION/","text":"Letters of Delegates to Congress: Data scraping This documentation outlines the process of scraping, cleaning, and storing data from the \"Letters of Delegates to Congress (1774-1789)\". The goal is to extract all relevant text data for further analysis, such as temporal network analysis and SHiCo analysis. Overview Objective The main objective is to scrape and save raw data from the \"Letters of Delegates to Congress (1774-1789)\" website hosted by the Library of Congress. This dataset will be used for various analytical purposes. Workflow Loading Required Libraries : Begin by loading the necessary R packages to facilitate web scraping and data manipulation. Scraping the Target Website : Extract URLs and corresponding text data from the Library of Congress website. Data Cleaning : Process the scraped text to remove unnecessary content and standardize the format. Matching ID to Text : Organize and structure the text data by linking it with relevant identifiers. Saving Raw Data : Store the cleaned data into files, either by individual volumes or as a combined dataset for further analysis. Step-by-Step Guide 1. Loading Required Libraries To start, load the essential libraries needed for web scraping and data manipulation: rvest : For web scraping. xml2 : To parse HTML content. tidyverse : For data wrangling and manipulation. These libraries enable the scraping of HTML content from the web, parsing the data, and organizing it for further analysis. 2. Scraping the Target Website The main task here is to scrape the text data from the \"Letters of Delegates to Congress\" available on the Library of Congress website. The process starts by accessing the base URL and extracting the relevant HTML nodes that contain the URLs to each volume of the letters. These URLs are then compiled into a list, which will be iterated over to scrape the text data. Each volume contains multiple documents, and the code captures the links to these documents, scrapes the text, and stores it for later use. 3. Data Cleaning Once the text data has been scraped, it requires cleaning to remove any extraneous content that is not relevant to the analysis. The cleaning process involves: Removing Unwanted Text : Stripping out repeated headers, footers, and navigation text that appear throughout the documents. Filtering Content : Excluding empty rows or rows with irrelevant data (e.g., \"Page\" headers). Standardizing Format : Ensuring that the remaining text is formatted consistently, which involves removing excess whitespace and duplicate rows. This cleaned text will then be structured for further analysis. 4. Matching ID to Text After cleaning, the data is further processed to match each piece of text with its corresponding identifier. This step involves organizing the text by assigning IDs to ensure each entry is correctly associated with its source. The cleaned and structured data will be stored in a tabular format, with the text content linked to its identifier. This structured format makes it easier to analyze and query the data, ensuring that each letter or document is correctly identified and accessible. 5. Saving Raw Data Finally, the cleaned and organized data is saved for future use. Depending on your needs, the data can be stored in individual files corresponding to each volume or combined into a single dataset: Individual Volume Files : If you need to analyze or review specific volumes, you can save each volume as a separate file. Combined Dataset : For broader analyses, all volumes can be merged into a single dataset and saved accordingly. The data is saved in .rds format, which preserves the structure and content of the data while being efficient in storage. This final step ensures that the raw data is ready for any further analysis you plan to perform. Conclusion This process allows you to efficiently scrape, clean, and store a large dataset from the \"Letters of Delegates to Congress\" collection. By following these steps, you ensure that the data is well-organized, consistent, and ready for complex analyses, such as temporal network analysis or SHiCo analysis.","title":"Scraping Letters of Delegates to Congress"},{"location":"Analyses/COLLECTION/#letters-of-delegates-to-congress-data-scraping","text":"This documentation outlines the process of scraping, cleaning, and storing data from the \"Letters of Delegates to Congress (1774-1789)\". The goal is to extract all relevant text data for further analysis, such as temporal network analysis and SHiCo analysis.","title":"Letters of Delegates to Congress: Data scraping"},{"location":"Analyses/COLLECTION/#overview","text":"","title":"Overview"},{"location":"Analyses/COLLECTION/#objective","text":"The main objective is to scrape and save raw data from the \"Letters of Delegates to Congress (1774-1789)\" website hosted by the Library of Congress. This dataset will be used for various analytical purposes.","title":"Objective"},{"location":"Analyses/COLLECTION/#workflow","text":"Loading Required Libraries : Begin by loading the necessary R packages to facilitate web scraping and data manipulation. Scraping the Target Website : Extract URLs and corresponding text data from the Library of Congress website. Data Cleaning : Process the scraped text to remove unnecessary content and standardize the format. Matching ID to Text : Organize and structure the text data by linking it with relevant identifiers. Saving Raw Data : Store the cleaned data into files, either by individual volumes or as a combined dataset for further analysis.","title":"Workflow"},{"location":"Analyses/COLLECTION/#step-by-step-guide","text":"","title":"Step-by-Step Guide"},{"location":"Analyses/COLLECTION/#1-loading-required-libraries","text":"To start, load the essential libraries needed for web scraping and data manipulation: rvest : For web scraping. xml2 : To parse HTML content. tidyverse : For data wrangling and manipulation. These libraries enable the scraping of HTML content from the web, parsing the data, and organizing it for further analysis.","title":"1. Loading Required Libraries"},{"location":"Analyses/COLLECTION/#2-scraping-the-target-website","text":"The main task here is to scrape the text data from the \"Letters of Delegates to Congress\" available on the Library of Congress website. The process starts by accessing the base URL and extracting the relevant HTML nodes that contain the URLs to each volume of the letters. These URLs are then compiled into a list, which will be iterated over to scrape the text data. Each volume contains multiple documents, and the code captures the links to these documents, scrapes the text, and stores it for later use.","title":"2. Scraping the Target Website"},{"location":"Analyses/COLLECTION/#3-data-cleaning","text":"Once the text data has been scraped, it requires cleaning to remove any extraneous content that is not relevant to the analysis. The cleaning process involves: Removing Unwanted Text : Stripping out repeated headers, footers, and navigation text that appear throughout the documents. Filtering Content : Excluding empty rows or rows with irrelevant data (e.g., \"Page\" headers). Standardizing Format : Ensuring that the remaining text is formatted consistently, which involves removing excess whitespace and duplicate rows. This cleaned text will then be structured for further analysis.","title":"3. Data Cleaning"},{"location":"Analyses/COLLECTION/#4-matching-id-to-text","text":"After cleaning, the data is further processed to match each piece of text with its corresponding identifier. This step involves organizing the text by assigning IDs to ensure each entry is correctly associated with its source. The cleaned and structured data will be stored in a tabular format, with the text content linked to its identifier. This structured format makes it easier to analyze and query the data, ensuring that each letter or document is correctly identified and accessible.","title":"4. Matching ID to Text"},{"location":"Analyses/COLLECTION/#5-saving-raw-data","text":"Finally, the cleaned and organized data is saved for future use. Depending on your needs, the data can be stored in individual files corresponding to each volume or combined into a single dataset: Individual Volume Files : If you need to analyze or review specific volumes, you can save each volume as a separate file. Combined Dataset : For broader analyses, all volumes can be merged into a single dataset and saved accordingly. The data is saved in .rds format, which preserves the structure and content of the data while being efficient in storage. This final step ensures that the raw data is ready for any further analysis you plan to perform.","title":"5. Saving Raw Data"},{"location":"Analyses/COLLECTION/#conclusion","text":"This process allows you to efficiently scrape, clean, and store a large dataset from the \"Letters of Delegates to Congress\" collection. By following these steps, you ensure that the data is well-organized, consistent, and ready for complex analyses, such as temporal network analysis or SHiCo analysis.","title":"Conclusion"},{"location":"Analyses/EXPLORE/","text":"Aggregated network visualization Overview This document details the workflow for plotting an aggregated network using data from the Letters of Delegates to Congress (1774-1789). The goal is to visualize the network of interactions among delegates, highlighting their communication patterns and connections. Workflow 1. Load Libraries Start by loading the necessary libraries: tidyverse : For data manipulation and visualization. igraph : For network analysis and plotting. 2. Load Preprocessed Data Import the preprocessed dataset using readRDS . This dataset, combined_df2 , contains the data required for plotting the network. 3. Plot Aggregated Network Select and Prepare Data Extract relevant columns ( sender_id and receiver_id ) from the dataset. Filter out rows with missing sender_id or receiver_id . Group the data by sender_id and receiver_id and calculate the weight of each edge by counting occurrences. Create and Simplify Network Convert the prepared data into a network object using igraph::graph_from_data_frame . Simplify the network by removing loops and multiple edges between the same nodes to streamline the visualization. Visualize Full Network Plot the entire network to observe the overall structure and connectivity. Focus on Subgraphs Create a subset of the network with nodes having a degree greater than a specified threshold (e.g., degree > 20). Define neighbors for main vertices and include them in an induced subgraph for focused analysis. Cluster Analysis Use the Louvain method to detect communities in the network. Apply modularity-based clustering to identify significant groups within the network. Assign community membership to vertices and plot the network with these clusters highlighted. Customize Visualization Create a custom color palette for different clusters. Calculate node strength, which reflects the sum of edge weights connected to each node, and adjust node sizes accordingly. Plot the network with customized vertex colors, sizes, and labels, and highlight specific groups if needed. 4. Save Network Plot Save the final visualization of the network plot to a file (implementation details to be added). Conclusion This workflow provides a comprehensive approach to visualizing and analyzing the network of interactions among delegates. By focusing on network structure, subgraphs, and community detection, it allows for a detailed exploration of communication patterns and relationships. The use of customized visual elements and clustering techniques enhances the clarity and interpretability of the network analysis, providing valuable insights into the connections between historical figures. Descriptive Analysis of Founders Online Dataset Overview This document provides a detailed workflow for conducting a descriptive analysis of the Founders Online dataset. The analysis focuses on understanding the corpus of letters, including author and recipient interactions, document counts, and temporal patterns. Workflow 1. Load Libraries Begin by loading the necessary library: tidyverse : For data manipulation and visualization. 2. Read Data Import the preprocessed datasets using readRDS : ffc5 : Contains the primary dataset for analysis. ffc6 : Contains additional data, possibly including more comprehensive records or extended periods. 3. Descriptive Analysis Background Information Create a reference table ( Period_table ) outlining the periods of interest, including: Period names (e.g., Colonial, Revolutionary War). General date ranges and specific start and end dates for each period. Unique Authors and Recipients Calculate the number of unique authors and recipients in the ffc5 dataset using n_distinct . Documents and Word Counts by Period Aggregate the number of documents and total word count by period using data.table::setDT and summarizing with by_period . Join the results with Period_table to enrich the data and format it for readability using knitr::kable . Temporal Analysis of Writings Frequency of Writings by Month : Filter and aggregate the number of documents written by notable founders each month. Plot the monthly frequency of writings, highlighting periods using vertical lines and faceting by author. Frequency of Received Writings by Month : Similarly, analyze and plot the frequency of received documents by founders over time. Adjust the visual details to highlight periods and facet by recipient. Interaction Between Founders Number of Messages Sent : Summarize and visualize the number of messages sent by each founder. Number of Messages Received : Summarize and visualize the number of messages received by each founder. 4. Save Plots and Results Save the generated plots and summary results (implementation details to be added). Conclusion This workflow provides a thorough descriptive analysis of the Founders Online dataset, focusing on document counts, author-recipient interactions, and temporal patterns. By following this process, you can gain valuable insights into the corpus of writings, track communication trends over time, and understand the volume and distribution of written exchanges among key historical figures.","title":"Exploratory analysis"},{"location":"Analyses/EXPLORE/#aggregated-network-visualization","text":"","title":"Aggregated network visualization"},{"location":"Analyses/EXPLORE/#overview","text":"This document details the workflow for plotting an aggregated network using data from the Letters of Delegates to Congress (1774-1789). The goal is to visualize the network of interactions among delegates, highlighting their communication patterns and connections.","title":"Overview"},{"location":"Analyses/EXPLORE/#workflow","text":"","title":"Workflow"},{"location":"Analyses/EXPLORE/#1-load-libraries","text":"Start by loading the necessary libraries: tidyverse : For data manipulation and visualization. igraph : For network analysis and plotting.","title":"1. Load Libraries"},{"location":"Analyses/EXPLORE/#2-load-preprocessed-data","text":"Import the preprocessed dataset using readRDS . This dataset, combined_df2 , contains the data required for plotting the network.","title":"2. Load Preprocessed Data"},{"location":"Analyses/EXPLORE/#3-plot-aggregated-network","text":"","title":"3. Plot Aggregated Network"},{"location":"Analyses/EXPLORE/#select-and-prepare-data","text":"Extract relevant columns ( sender_id and receiver_id ) from the dataset. Filter out rows with missing sender_id or receiver_id . Group the data by sender_id and receiver_id and calculate the weight of each edge by counting occurrences.","title":"Select and Prepare Data"},{"location":"Analyses/EXPLORE/#create-and-simplify-network","text":"Convert the prepared data into a network object using igraph::graph_from_data_frame . Simplify the network by removing loops and multiple edges between the same nodes to streamline the visualization.","title":"Create and Simplify Network"},{"location":"Analyses/EXPLORE/#visualize-full-network","text":"Plot the entire network to observe the overall structure and connectivity.","title":"Visualize Full Network"},{"location":"Analyses/EXPLORE/#focus-on-subgraphs","text":"Create a subset of the network with nodes having a degree greater than a specified threshold (e.g., degree > 20). Define neighbors for main vertices and include them in an induced subgraph for focused analysis.","title":"Focus on Subgraphs"},{"location":"Analyses/EXPLORE/#cluster-analysis","text":"Use the Louvain method to detect communities in the network. Apply modularity-based clustering to identify significant groups within the network. Assign community membership to vertices and plot the network with these clusters highlighted.","title":"Cluster Analysis"},{"location":"Analyses/EXPLORE/#customize-visualization","text":"Create a custom color palette for different clusters. Calculate node strength, which reflects the sum of edge weights connected to each node, and adjust node sizes accordingly. Plot the network with customized vertex colors, sizes, and labels, and highlight specific groups if needed.","title":"Customize Visualization"},{"location":"Analyses/EXPLORE/#4-save-network-plot","text":"Save the final visualization of the network plot to a file (implementation details to be added).","title":"4. Save Network Plot"},{"location":"Analyses/EXPLORE/#conclusion","text":"This workflow provides a comprehensive approach to visualizing and analyzing the network of interactions among delegates. By focusing on network structure, subgraphs, and community detection, it allows for a detailed exploration of communication patterns and relationships. The use of customized visual elements and clustering techniques enhances the clarity and interpretability of the network analysis, providing valuable insights into the connections between historical figures.","title":"Conclusion"},{"location":"Analyses/EXPLORE/#descriptive-analysis-of-founders-online-dataset","text":"","title":"Descriptive Analysis of Founders Online Dataset"},{"location":"Analyses/EXPLORE/#overview_1","text":"This document provides a detailed workflow for conducting a descriptive analysis of the Founders Online dataset. The analysis focuses on understanding the corpus of letters, including author and recipient interactions, document counts, and temporal patterns.","title":"Overview"},{"location":"Analyses/EXPLORE/#workflow_1","text":"","title":"Workflow"},{"location":"Analyses/EXPLORE/#1-load-libraries_1","text":"Begin by loading the necessary library: tidyverse : For data manipulation and visualization.","title":"1. Load Libraries"},{"location":"Analyses/EXPLORE/#2-read-data","text":"Import the preprocessed datasets using readRDS : ffc5 : Contains the primary dataset for analysis. ffc6 : Contains additional data, possibly including more comprehensive records or extended periods.","title":"2. Read Data"},{"location":"Analyses/EXPLORE/#3-descriptive-analysis","text":"","title":"3. Descriptive Analysis"},{"location":"Analyses/EXPLORE/#background-information","text":"Create a reference table ( Period_table ) outlining the periods of interest, including: Period names (e.g., Colonial, Revolutionary War). General date ranges and specific start and end dates for each period.","title":"Background Information"},{"location":"Analyses/EXPLORE/#unique-authors-and-recipients","text":"Calculate the number of unique authors and recipients in the ffc5 dataset using n_distinct .","title":"Unique Authors and Recipients"},{"location":"Analyses/EXPLORE/#documents-and-word-counts-by-period","text":"Aggregate the number of documents and total word count by period using data.table::setDT and summarizing with by_period . Join the results with Period_table to enrich the data and format it for readability using knitr::kable .","title":"Documents and Word Counts by Period"},{"location":"Analyses/EXPLORE/#temporal-analysis-of-writings","text":"Frequency of Writings by Month : Filter and aggregate the number of documents written by notable founders each month. Plot the monthly frequency of writings, highlighting periods using vertical lines and faceting by author. Frequency of Received Writings by Month : Similarly, analyze and plot the frequency of received documents by founders over time. Adjust the visual details to highlight periods and facet by recipient.","title":"Temporal Analysis of Writings"},{"location":"Analyses/EXPLORE/#interaction-between-founders","text":"Number of Messages Sent : Summarize and visualize the number of messages sent by each founder. Number of Messages Received : Summarize and visualize the number of messages received by each founder.","title":"Interaction Between Founders"},{"location":"Analyses/EXPLORE/#4-save-plots-and-results","text":"Save the generated plots and summary results (implementation details to be added).","title":"4. Save Plots and Results"},{"location":"Analyses/EXPLORE/#conclusion_1","text":"This workflow provides a thorough descriptive analysis of the Founders Online dataset, focusing on document counts, author-recipient interactions, and temporal patterns. By following this process, you can gain valuable insights into the corpus of writings, track communication trends over time, and understand the volume and distribution of written exchanges among key historical figures.","title":"Conclusion"},{"location":"Analyses/FFCOLLECTION/","text":"Founding Fathers Letters: Data scraping This documentation outlines the process for scraping, cleaning, and saving data from the \"Founding Fathers\" website, available at https://founders.archives.gov/ . This dataset is utilized for historical research and analysis. The modified code provided by Jay Timm, which can be found here , has been adapted for this purpose. Overview Objective The goal is to scrape and save text data from the \"Founding Fathers\" website, extracting metadata and content from XML and JSON sources. The data will be cleaned and organized into a structured format suitable for further analysis. Workflow Load Required Libraries : Import the necessary R packages for data manipulation and scraping. Scrape Data : Extract metadata and text content from XML and JSON files. Save Data : Organize and save the cleaned data into files categorized by historical periods. Step-by-Step Guide 1. Load Required Libraries First, load the tidyverse library, which includes essential tools for data manipulation and scraping. This library facilitates various data wrangling tasks. library(tidyverse) Set the working directory to where your XML metadata file is located and specify the file name. 2. Scrape Data Extract Metadata To obtain metadata for each letter, read the XML file using the xml2::read_xml() function. This function parses the XML file, allowing you to extract key metadata fields such as title, permalink, project, authors, recipients, and dates. These fields are then compiled into a data frame. Variable : xml2 Function : xml2::as_list() Extract Text Content The full-text content of each letter is retrieved through the provided API. Use the jsonlite::fromJSON() function to parse the JSON responses from the API. A custom function, clean_text() , is applied to process the raw text, removing unnecessary whitespace and empty lines. Variable : x2$api Function : jsonlite::fromJSON() Function : clean_text() Categorize Texts by Historical Period Assign each letter to a historical period based on its date using the mutate() function along with the case_when() helper from the dplyr package. Historical periods include Colonial, Revolutionary War, Confederation Period, and various presidencies. Function : mutate() Function : case_when() 3. Save Data The cleaned and categorized data is saved into separate RDS files, one for each historical period. Use the saveRDS() function to serialize the data frames into RDS files, which are suitable for storage and further analysis. Function : saveRDS() Set the working directory to the desired location for saving files. Utilize lapply() to iterate through each period\u2019s data and save it as individual RDS files. Conclusion By following these steps, you can efficiently scrape, clean, and save text data from the \"Founding Fathers\" website. This process ensures that the data is well-organized and ready for further historical analysis.","title":"Scraping Founding Fathers"},{"location":"Analyses/FFCOLLECTION/#founding-fathers-letters-data-scraping","text":"This documentation outlines the process for scraping, cleaning, and saving data from the \"Founding Fathers\" website, available at https://founders.archives.gov/ . This dataset is utilized for historical research and analysis. The modified code provided by Jay Timm, which can be found here , has been adapted for this purpose.","title":"Founding Fathers Letters: Data scraping"},{"location":"Analyses/FFCOLLECTION/#overview","text":"","title":"Overview"},{"location":"Analyses/FFCOLLECTION/#objective","text":"The goal is to scrape and save text data from the \"Founding Fathers\" website, extracting metadata and content from XML and JSON sources. The data will be cleaned and organized into a structured format suitable for further analysis.","title":"Objective"},{"location":"Analyses/FFCOLLECTION/#workflow","text":"Load Required Libraries : Import the necessary R packages for data manipulation and scraping. Scrape Data : Extract metadata and text content from XML and JSON files. Save Data : Organize and save the cleaned data into files categorized by historical periods.","title":"Workflow"},{"location":"Analyses/FFCOLLECTION/#step-by-step-guide","text":"","title":"Step-by-Step Guide"},{"location":"Analyses/FFCOLLECTION/#1-load-required-libraries","text":"First, load the tidyverse library, which includes essential tools for data manipulation and scraping. This library facilitates various data wrangling tasks. library(tidyverse) Set the working directory to where your XML metadata file is located and specify the file name.","title":"1. Load Required Libraries"},{"location":"Analyses/FFCOLLECTION/#2-scrape-data","text":"","title":"2. Scrape Data"},{"location":"Analyses/FFCOLLECTION/#extract-metadata","text":"To obtain metadata for each letter, read the XML file using the xml2::read_xml() function. This function parses the XML file, allowing you to extract key metadata fields such as title, permalink, project, authors, recipients, and dates. These fields are then compiled into a data frame. Variable : xml2 Function : xml2::as_list()","title":"Extract Metadata"},{"location":"Analyses/FFCOLLECTION/#extract-text-content","text":"The full-text content of each letter is retrieved through the provided API. Use the jsonlite::fromJSON() function to parse the JSON responses from the API. A custom function, clean_text() , is applied to process the raw text, removing unnecessary whitespace and empty lines. Variable : x2$api Function : jsonlite::fromJSON() Function : clean_text()","title":"Extract Text Content"},{"location":"Analyses/FFCOLLECTION/#categorize-texts-by-historical-period","text":"Assign each letter to a historical period based on its date using the mutate() function along with the case_when() helper from the dplyr package. Historical periods include Colonial, Revolutionary War, Confederation Period, and various presidencies. Function : mutate() Function : case_when()","title":"Categorize Texts by Historical Period"},{"location":"Analyses/FFCOLLECTION/#3-save-data","text":"The cleaned and categorized data is saved into separate RDS files, one for each historical period. Use the saveRDS() function to serialize the data frames into RDS files, which are suitable for storage and further analysis. Function : saveRDS() Set the working directory to the desired location for saving files. Utilize lapply() to iterate through each period\u2019s data and save it as individual RDS files.","title":"3. Save Data"},{"location":"Analyses/FFCOLLECTION/#conclusion","text":"By following these steps, you can efficiently scrape, clean, and save text data from the \"Founding Fathers\" website. This process ensures that the data is well-organized and ready for further historical analysis.","title":"Conclusion"},{"location":"Analyses/INTRODUCTION/","text":"Introduction The code for the analyses can be found in the subfolder scripts . It is structured in parallel to the structure explained above: 1_data_collection 2_data_preparation 3_exploratory_data_analysis In adition, the code and procedures to create the word2vec models needed as input for running shico can be found in the subfolder w2v_models/delegates . The code is applied to the data from the delegates of congress, but can be easily adapted and eqaully applied to create word2vec models for all other datasets in this project as well.","title":"Introduction"},{"location":"Analyses/INTRODUCTION/#introduction","text":"The code for the analyses can be found in the subfolder scripts . It is structured in parallel to the structure explained above: 1_data_collection 2_data_preparation 3_exploratory_data_analysis In adition, the code and procedures to create the word2vec models needed as input for running shico can be found in the subfolder w2v_models/delegates . The code is applied to the data from the delegates of congress, but can be easily adapted and eqaully applied to create word2vec models for all other datasets in this project as well.","title":"Introduction"},{"location":"Analyses/PREPARATION_DEL/","text":"Preprocessing raw data on Letters of Delegates to Congress 0. GOAL The script aims to preprocess raw data on Letters of Delegates to Congress (1774-1789) for future temporal network and SHICO analysis. 1. LOAD IN LIBRARIES library(tidyverse) # For data wrangling library(polyglotr) # For text translation and manipulation 2. DATA PREPROCESSING A) Volume and Year Variables Objective: Extract and clean the volume number, period, and year from the dataset. Steps: 1. Split the text column to separate Volume and Content. 2. Extract volume number and period from the Volume column. 3. Extract the year from the Content column. 4. Handle cases where the title is not present and clean it. B) Improve Information on Year Objective: Correct and fill in missing or incorrect year information. Steps: 1. Define a function to extract year indices from text. 2. Loop through each row to verify and update the Year column based on extracted year indices. C) Title, From, and To Variables Objective: Clean and standardize title, sender, and recipient information. Steps: 1. Use predefined separators to handle titles without proper delimiters. 2. Clean the title by removing specific words, symbols, and correcting anomalies. 3. Separate titles into From and To columns based on the presence of \" to \". 4. Remove or update unknown sender or recipient information. D) Letter Contents Variable Objective: Clean and format the content of letters. Steps: 1. Remove editor notes and special characters. 2. Use a function to display letters with author and recipient details. 3. Clean Content2 by removing unwanted text and special characters. E) Date Variable Objective: Extract and standardize date information from letter content. Steps: 1. Define a function to clean and process date information. 2. Standardize date formats and handle manual corrections for specific IDs. 3. Convert the cleaned dates to Date format and filter data based on the specified date range. 3. SAVING PREPROCESSED DATA A1) All Data in CSV File Objective: Save the preprocessed dataset to a CSV file. Steps: 1. Select relevant columns and rename them. 2. Convert column names to lowercase and write the data to a CSV file. A2) All Data in RDS File Objective: Save the preprocessed dataset to an RDS file. Steps: 1. Save the data to an RDS file for further use. B) For Making Word2Vec Models (SHICO) Objective: Prepare a dataset for Word2Vec model building. Steps: 1. Select relevant columns and save them to a CSV file. C) Letters as Separate Text Files Objective: Save each letter as a separate text file for further processing. Steps: 1. Loop through volume numbers, read RDS files, clean text, and save each letter as a text file in a corresponding folder. D) Letters Per Year Objective: Split data by year and save to CSV files. Steps: 1. Define a time period and filter data by year (commented out in the script). Note: Standard cleaning of letter texts for Word2Vec models is performed in a separate Python script ( make_word2vec_models_for_delegates.py ), found in w2v_models/delegates . This script preprocesses the data to develop Word2Vec models for SHICO analysis.","title":"Data preparation Delegates of Congres"},{"location":"Analyses/PREPARATION_DEL/#preprocessing-raw-data-on-letters-of-delegates-to-congress","text":"","title":"Preprocessing raw data on Letters of Delegates to Congress"},{"location":"Analyses/PREPARATION_DEL/#0-goal","text":"The script aims to preprocess raw data on Letters of Delegates to Congress (1774-1789) for future temporal network and SHICO analysis.","title":"0. GOAL"},{"location":"Analyses/PREPARATION_DEL/#1-load-in-libraries","text":"library(tidyverse) # For data wrangling library(polyglotr) # For text translation and manipulation","title":"1. LOAD IN LIBRARIES"},{"location":"Analyses/PREPARATION_DEL/#2-data-preprocessing","text":"","title":"2. DATA PREPROCESSING"},{"location":"Analyses/PREPARATION_DEL/#a-volume-and-year-variables","text":"Objective: Extract and clean the volume number, period, and year from the dataset. Steps: 1. Split the text column to separate Volume and Content. 2. Extract volume number and period from the Volume column. 3. Extract the year from the Content column. 4. Handle cases where the title is not present and clean it.","title":"A) Volume and Year Variables"},{"location":"Analyses/PREPARATION_DEL/#b-improve-information-on-year","text":"Objective: Correct and fill in missing or incorrect year information. Steps: 1. Define a function to extract year indices from text. 2. Loop through each row to verify and update the Year column based on extracted year indices.","title":"B) Improve Information on Year"},{"location":"Analyses/PREPARATION_DEL/#c-title-from-and-to-variables","text":"Objective: Clean and standardize title, sender, and recipient information. Steps: 1. Use predefined separators to handle titles without proper delimiters. 2. Clean the title by removing specific words, symbols, and correcting anomalies. 3. Separate titles into From and To columns based on the presence of \" to \". 4. Remove or update unknown sender or recipient information.","title":"C) Title, From, and To Variables"},{"location":"Analyses/PREPARATION_DEL/#d-letter-contents-variable","text":"Objective: Clean and format the content of letters. Steps: 1. Remove editor notes and special characters. 2. Use a function to display letters with author and recipient details. 3. Clean Content2 by removing unwanted text and special characters.","title":"D) Letter Contents Variable"},{"location":"Analyses/PREPARATION_DEL/#e-date-variable","text":"Objective: Extract and standardize date information from letter content. Steps: 1. Define a function to clean and process date information. 2. Standardize date formats and handle manual corrections for specific IDs. 3. Convert the cleaned dates to Date format and filter data based on the specified date range.","title":"E) Date Variable"},{"location":"Analyses/PREPARATION_DEL/#3-saving-preprocessed-data","text":"","title":"3. SAVING PREPROCESSED DATA"},{"location":"Analyses/PREPARATION_DEL/#a1-all-data-in-csv-file","text":"Objective: Save the preprocessed dataset to a CSV file. Steps: 1. Select relevant columns and rename them. 2. Convert column names to lowercase and write the data to a CSV file.","title":"A1) All Data in CSV File"},{"location":"Analyses/PREPARATION_DEL/#a2-all-data-in-rds-file","text":"Objective: Save the preprocessed dataset to an RDS file. Steps: 1. Save the data to an RDS file for further use.","title":"A2) All Data in RDS File"},{"location":"Analyses/PREPARATION_DEL/#b-for-making-word2vec-models-shico","text":"Objective: Prepare a dataset for Word2Vec model building. Steps: 1. Select relevant columns and save them to a CSV file.","title":"B) For Making Word2Vec Models (SHICO)"},{"location":"Analyses/PREPARATION_DEL/#c-letters-as-separate-text-files","text":"Objective: Save each letter as a separate text file for further processing. Steps: 1. Loop through volume numbers, read RDS files, clean text, and save each letter as a text file in a corresponding folder.","title":"C) Letters as Separate Text Files"},{"location":"Analyses/PREPARATION_DEL/#d-letters-per-year","text":"Objective: Split data by year and save to CSV files. Steps: 1. Define a time period and filter data by year (commented out in the script). Note: Standard cleaning of letter texts for Word2Vec models is performed in a separate Python script ( make_word2vec_models_for_delegates.py ), found in w2v_models/delegates . This script preprocesses the data to develop Word2Vec models for SHICO analysis.","title":"D) Letters Per Year"},{"location":"Analyses/PREPARATION_FF/","text":"Preprocessing raw data on Letters of Founding Fathers Workflow Overview The following documentation describes the step-by-step workflow for analyzing a dataset in R. This workflow involves several key phases, including data loading, preprocessing, analysis, and visualization. The aim is to provide a clear and efficient approach to handling and analyzing data using R. Step-by-Step Guide 1. Data Loading Objective: Import the dataset into R for further analysis. Use appropriate functions to load data from various formats (e.g., CSV, Excel). Ensure data is read correctly by examining the structure and initial rows of the dataset. 2. Data Preprocessing Objective: Clean and prepare the data for analysis. Handling Missing Values: Identify and manage missing values. Options include removing rows with missing data or imputing values based on the context. Data Transformation: Convert data types if necessary (e.g., changing factors to numeric values). Normalization: Scale or normalize data if required for specific analyses. 3. Data Exploration Objective: Perform exploratory data analysis (EDA) to understand the dataset's characteristics. Summary Statistics: Calculate summary statistics (mean, median, standard deviation) to get a sense of the data distribution. Visual Inspection: Create visualizations such as histograms, boxplots, and scatter plots to identify patterns, trends, and potential outliers. 4. Data Analysis Objective: Apply statistical or machine learning methods to extract insights from the data. Statistical Analysis: Conduct statistical tests or calculations to examine relationships and hypotheses. Model Building: Develop and train models if predictive analysis is required. Evaluate model performance using appropriate metrics. 5. Visualization Objective: Create visual representations of the data and analysis results. Charts and Graphs: Generate charts (e.g., bar plots, line graphs) to visualize trends and comparisons. Advanced Visualizations: Use more complex visualizations (e.g., heatmaps, network graphs) if needed to convey detailed insights. 6. Reporting Objective: Summarize the findings and provide actionable insights. Results Interpretation: Summarize the main findings from the analysis and interpret them in the context of the research question. Documentation: Document the analysis process, results, and any conclusions drawn. Ensure clarity and completeness in the reporting. Conclusion This workflow outlines a comprehensive approach to data analysis using R, from initial data loading to final reporting. By following these steps, analysts can ensure a systematic and effective analysis process, leading to meaningful insights and well-documented results. Consistent application of these steps will enhance the reproducibility and reliability of data analysis projects.","title":"Data preparation Founding Fathers"},{"location":"Analyses/PREPARATION_FF/#preprocessing-raw-data-on-letters-of-founding-fathers","text":"","title":"Preprocessing raw data on Letters of Founding Fathers"},{"location":"Analyses/PREPARATION_FF/#workflow-overview","text":"The following documentation describes the step-by-step workflow for analyzing a dataset in R. This workflow involves several key phases, including data loading, preprocessing, analysis, and visualization. The aim is to provide a clear and efficient approach to handling and analyzing data using R.","title":"Workflow Overview"},{"location":"Analyses/PREPARATION_FF/#step-by-step-guide","text":"","title":"Step-by-Step Guide"},{"location":"Analyses/PREPARATION_FF/#1-data-loading","text":"Objective: Import the dataset into R for further analysis. Use appropriate functions to load data from various formats (e.g., CSV, Excel). Ensure data is read correctly by examining the structure and initial rows of the dataset.","title":"1. Data Loading"},{"location":"Analyses/PREPARATION_FF/#2-data-preprocessing","text":"Objective: Clean and prepare the data for analysis. Handling Missing Values: Identify and manage missing values. Options include removing rows with missing data or imputing values based on the context. Data Transformation: Convert data types if necessary (e.g., changing factors to numeric values). Normalization: Scale or normalize data if required for specific analyses.","title":"2. Data Preprocessing"},{"location":"Analyses/PREPARATION_FF/#3-data-exploration","text":"Objective: Perform exploratory data analysis (EDA) to understand the dataset's characteristics. Summary Statistics: Calculate summary statistics (mean, median, standard deviation) to get a sense of the data distribution. Visual Inspection: Create visualizations such as histograms, boxplots, and scatter plots to identify patterns, trends, and potential outliers.","title":"3. Data Exploration"},{"location":"Analyses/PREPARATION_FF/#4-data-analysis","text":"Objective: Apply statistical or machine learning methods to extract insights from the data. Statistical Analysis: Conduct statistical tests or calculations to examine relationships and hypotheses. Model Building: Develop and train models if predictive analysis is required. Evaluate model performance using appropriate metrics.","title":"4. Data Analysis"},{"location":"Analyses/PREPARATION_FF/#5-visualization","text":"Objective: Create visual representations of the data and analysis results. Charts and Graphs: Generate charts (e.g., bar plots, line graphs) to visualize trends and comparisons. Advanced Visualizations: Use more complex visualizations (e.g., heatmaps, network graphs) if needed to convey detailed insights.","title":"5. Visualization"},{"location":"Analyses/PREPARATION_FF/#6-reporting","text":"Objective: Summarize the findings and provide actionable insights. Results Interpretation: Summarize the main findings from the analysis and interpret them in the context of the research question. Documentation: Document the analysis process, results, and any conclusions drawn. Ensure clarity and completeness in the reporting.","title":"6. Reporting"},{"location":"Analyses/PREPARATION_FF/#conclusion","text":"This workflow outlines a comprehensive approach to data analysis using R, from initial data loading to final reporting. By following these steps, analysts can ensure a systematic and effective analysis process, leading to meaningful insights and well-documented results. Consistent application of these steps will enhance the reproducibility and reliability of data analysis projects.","title":"Conclusion"},{"location":"Analyses/WORD2VEC/","text":"Text processing and word2vec model training Overview This script performs various text processing tasks on a dataset of letters of the Delegates of Congres, including: Extracting letter content from raw files. Tokenizing and cleaning text data. Training Word2Vec models on the processed text. 1. Libraries The script requires several libraries for its operations: argparse : For command-line argument parsing. pandas : For data manipulation and reading CSV files. tqdm : For progress bars. numpy : For numerical operations. nltk : For natural language processing (tokenization and stopword removal). string : For string operations. gensim : For topic modeling and Word2Vec model training. os : For file and directory operations. csv : For CSV file operations. 2. Check System Architecture The script adjusts the CSV field size limit based on whether the system is 32-bit or 64-bit. 3. Functions get_content(args) Purpose : Extracts the content of letters from text files and saves them into CSV files by year. Parameters : args (command-line arguments, though not used in the function). Process : Reads metadata from Letters.csv . Retrieves the content from text files based on the 'TCP' field. Saves each year's content to a separate CSV file. split_into_sentences(text) Purpose : Splits text into sentences. Parameters : text (string). Returns : List of sentences. split_into_words(text) Purpose : Tokenizes sentences into words. Parameters : text (list of sentences). Returns : List of lists of words. remove_stopwords_and_punctuation(text, stopwords) Purpose : Removes stopwords and punctuation from tokenized text. Parameters : text (list of lists of words). stopwords (list of stopwords). Returns : Cleaned list of lists of words. join_words(text) Purpose : Joins lists of words into a single string document. Parameters : text (list of lists of words). Returns : String document. tokenize(args) Purpose : Processes raw text files into tokenized and cleaned text, and saves them in CSV files. Parameters : args (command-line arguments, though not used in the function). Process : Reads text content from CSV files. Tokenizes, removes stopwords, and joins words. Saves the processed text to new CSV files. get_min_max_year() Purpose : Retrieves the minimum and maximum year from tokenized data. Returns : Tuple of (min_year, max_year). get_sentences_for_year(year) Purpose : Retrieves tokenized sentences from a CSV file for a specific year. Parameters : year (int). Returns : List of lists of sentences. get_sentences_in_range(start_y, end_y) Purpose : Retrieves tokenized sentences for a range of years. Parameters : start_y (int): Start year. end_y (int): End year. Returns : List of lists of sentences. train(args) Purpose : Trains Word2Vec models on text data from specified year ranges. Parameters : args (command-line arguments, includes optional window size for model training). Process : Constructs and trains a Word2Vec model for each year range. Saves the trained model to disk. 4. Main Function Purpose : Parses command-line arguments and executes the corresponding function ( get_content , tokenize , or train ). Command-line Arguments : content : Extracts content from raw data. tokenize : Tokenizes and cleans text data. train : Trains Word2Vec models, with an optional window size argument. Conclusion This script provides a comprehensive solution for processing and analyzing historical letter data. It extracts and tokenizes text content, prepares it for modeling, and trains Word2Vec models to capture semantic relationships between words.","title":"Word2Vec models"},{"location":"Analyses/WORD2VEC/#text-processing-and-word2vec-model-training","text":"","title":"Text processing and word2vec model training"},{"location":"Analyses/WORD2VEC/#overview","text":"This script performs various text processing tasks on a dataset of letters of the Delegates of Congres, including: Extracting letter content from raw files. Tokenizing and cleaning text data. Training Word2Vec models on the processed text.","title":"Overview"},{"location":"Analyses/WORD2VEC/#1-libraries","text":"The script requires several libraries for its operations: argparse : For command-line argument parsing. pandas : For data manipulation and reading CSV files. tqdm : For progress bars. numpy : For numerical operations. nltk : For natural language processing (tokenization and stopword removal). string : For string operations. gensim : For topic modeling and Word2Vec model training. os : For file and directory operations. csv : For CSV file operations.","title":"1. Libraries"},{"location":"Analyses/WORD2VEC/#2-check-system-architecture","text":"The script adjusts the CSV field size limit based on whether the system is 32-bit or 64-bit.","title":"2. Check System Architecture"},{"location":"Analyses/WORD2VEC/#3-functions","text":"","title":"3. Functions"},{"location":"Analyses/WORD2VEC/#get_contentargs","text":"Purpose : Extracts the content of letters from text files and saves them into CSV files by year. Parameters : args (command-line arguments, though not used in the function). Process : Reads metadata from Letters.csv . Retrieves the content from text files based on the 'TCP' field. Saves each year's content to a separate CSV file.","title":"get_content(args)"},{"location":"Analyses/WORD2VEC/#split_into_sentencestext","text":"Purpose : Splits text into sentences. Parameters : text (string). Returns : List of sentences.","title":"split_into_sentences(text)"},{"location":"Analyses/WORD2VEC/#split_into_wordstext","text":"Purpose : Tokenizes sentences into words. Parameters : text (list of sentences). Returns : List of lists of words.","title":"split_into_words(text)"},{"location":"Analyses/WORD2VEC/#remove_stopwords_and_punctuationtext-stopwords","text":"Purpose : Removes stopwords and punctuation from tokenized text. Parameters : text (list of lists of words). stopwords (list of stopwords). Returns : Cleaned list of lists of words.","title":"remove_stopwords_and_punctuation(text, stopwords)"},{"location":"Analyses/WORD2VEC/#join_wordstext","text":"Purpose : Joins lists of words into a single string document. Parameters : text (list of lists of words). Returns : String document.","title":"join_words(text)"},{"location":"Analyses/WORD2VEC/#tokenizeargs","text":"Purpose : Processes raw text files into tokenized and cleaned text, and saves them in CSV files. Parameters : args (command-line arguments, though not used in the function). Process : Reads text content from CSV files. Tokenizes, removes stopwords, and joins words. Saves the processed text to new CSV files.","title":"tokenize(args)"},{"location":"Analyses/WORD2VEC/#get_min_max_year","text":"Purpose : Retrieves the minimum and maximum year from tokenized data. Returns : Tuple of (min_year, max_year).","title":"get_min_max_year()"},{"location":"Analyses/WORD2VEC/#get_sentences_for_yearyear","text":"Purpose : Retrieves tokenized sentences from a CSV file for a specific year. Parameters : year (int). Returns : List of lists of sentences.","title":"get_sentences_for_year(year)"},{"location":"Analyses/WORD2VEC/#get_sentences_in_rangestart_y-end_y","text":"Purpose : Retrieves tokenized sentences for a range of years. Parameters : start_y (int): Start year. end_y (int): End year. Returns : List of lists of sentences.","title":"get_sentences_in_range(start_y, end_y)"},{"location":"Analyses/WORD2VEC/#trainargs","text":"Purpose : Trains Word2Vec models on text data from specified year ranges. Parameters : args (command-line arguments, includes optional window size for model training). Process : Constructs and trains a Word2Vec model for each year range. Saves the trained model to disk.","title":"train(args)"},{"location":"Analyses/WORD2VEC/#4-main-function","text":"Purpose : Parses command-line arguments and executes the corresponding function ( get_content , tokenize , or train ). Command-line Arguments : content : Extracts content from raw data. tokenize : Tokenizes and cleans text data. train : Trains Word2Vec models, with an optional window size argument.","title":"4. Main Function"},{"location":"Analyses/WORD2VEC/#conclusion","text":"This script provides a comprehensive solution for processing and analyzing historical letter data. It extracts and tokenizes text content, prepares it for modeling, and trains Word2Vec models to capture semantic relationships between words.","title":"Conclusion"},{"location":"Background/BACKGROUND/","text":"The H-GEAR project The H-GEAR project is unraveling the intellectual tapestry of America's founders by using digital methods to chart shifts in the character and structure of political discourse during the American Revolution. The project is centered on a large database of time-stamped letters that were exchanged between seven so-called \u201cfounding fathers\u201d and other key figures of that time. The ambition of the project is to understand how political discourse shifted during the Revolutionary era in the United States, to reveal which people had influence and power, and to discover where and how these changes spread. 1) The first phase of the project involves content-based analysis with word2vec-based Shico software . We aim to trace how liberal and republican concepts and ideologies shifted in the period between 1725-1835, uncovering the subtle nuances in language usage that shaped political discourse. Given a small set of user-provided keywords (e.g., freedom, liberty, right), ShiCo gives insight into changes in the vocabulary used to denote the concept by examining the semantic relations between words in different years. 2) The second phase employs the combination of semi-supervised topic modeling with seededLDA and temporal social network analysis with pathpy to establish links between the writers through time. Through the analysis of letters exchanged, the project seeks to identify the most influential figures and map the network that propagated these political ideologies. Fndings underline that Thomas Jefferson emerges as the central figure of this period \u2013 not surprising given his leading role in drafting the Declaration of Independence. However, as is common to historical network analyses, because the corpus of letters is biased with its focus on the founding fathers, the researchers acknowledge that the results are equally biased. With anticipation of the 250th anniversary of the Declaration of Independence in 2026, we believe interest in the American Revolution will garner even more attention. Research questions How did the political discourse centered around the two concepts of liberal and republican ideologies develop during the Revolutionary era? Who initiated and drove the spread of these ideas on political ideology? How did these ideas on political ideologies spread accross the network of interconnected people? Can we identify specific moments in time and places where these shifts are particularly prominent?","title":"Hgear"},{"location":"Background/BACKGROUND/#the-h-gear-project","text":"The H-GEAR project is unraveling the intellectual tapestry of America's founders by using digital methods to chart shifts in the character and structure of political discourse during the American Revolution. The project is centered on a large database of time-stamped letters that were exchanged between seven so-called \u201cfounding fathers\u201d and other key figures of that time. The ambition of the project is to understand how political discourse shifted during the Revolutionary era in the United States, to reveal which people had influence and power, and to discover where and how these changes spread. 1) The first phase of the project involves content-based analysis with word2vec-based Shico software . We aim to trace how liberal and republican concepts and ideologies shifted in the period between 1725-1835, uncovering the subtle nuances in language usage that shaped political discourse. Given a small set of user-provided keywords (e.g., freedom, liberty, right), ShiCo gives insight into changes in the vocabulary used to denote the concept by examining the semantic relations between words in different years. 2) The second phase employs the combination of semi-supervised topic modeling with seededLDA and temporal social network analysis with pathpy to establish links between the writers through time. Through the analysis of letters exchanged, the project seeks to identify the most influential figures and map the network that propagated these political ideologies. Fndings underline that Thomas Jefferson emerges as the central figure of this period \u2013 not surprising given his leading role in drafting the Declaration of Independence. However, as is common to historical network analyses, because the corpus of letters is biased with its focus on the founding fathers, the researchers acknowledge that the results are equally biased. With anticipation of the 250th anniversary of the Declaration of Independence in 2026, we believe interest in the American Revolution will garner even more attention.","title":"The H-GEAR project"},{"location":"Background/BACKGROUND/#research-questions","text":"How did the political discourse centered around the two concepts of liberal and republican ideologies develop during the Revolutionary era? Who initiated and drove the spread of these ideas on political ideology? How did these ideas on political ideologies spread accross the network of interconnected people? Can we identify specific moments in time and places where these shifts are particularly prominent?","title":"Research questions"},{"location":"Background/DATA/","text":"The data This project is centered on four datasets: Founders online This project examined the entire corpus of 185.000 documents in the US National Archives\u2019 Founders Online database. This database brings together the bulk of the edited and collected papers of seven so-called Founding Fathers from existing print editions as well as from additional online-only sources; a total of 294 print volumes. The archive is a rich digital collection of primary source documents from the most prominent figures in early American history, including George Washington, John Adams, Thomas Jefferson, Alexander Hamilton, and James Madison. The database also includes the papers of Benjamin Franklin, John Jay, and the records of the Continental Congress. The documents are fully searchable and are available in both digital facsimile and transcribed formats. For a description of the Founders Online project and its contents, see the \u201cAbout\u201d page (https://founders.archives.gov/about) and the complete list of volumes (https://founders.archives.gov/content/volumes). The data was scraped from the Founders Online website, and can be retrieved here: https://github.com/jaytimm/founders-online-corpus. For more recently scraped data, go to https://github.com/h-gear/Explore_with_R/tree/main/data Letters of Delegates to Congress This collection of 25 volumes contains all public laws, congressional debates, and (to a limited extent) correspondence of members of Congress for the period of 1774-1789. Letters from delegates comprise most of the entries, although there are also (parts of) public papers, essays, and diaries. In total, ca. 15.000 documents are included. The data was scraped from the Library of Congress' American Memory project. Further information on this data can be found here: https://memory.loc.gov/ammem/amlaw/lwdg.html The corresponding data can be retrieved here: https://github.com/h-gear/revolution/tree/main/data/processed/delegates Eighteenth Century Collections Online (ECCO) TCP The Eighteenth Century Collections Online (ECCO) TCP contains about 3100 keyed and searchable published texts from the Eighteenth Century Collections Online database (ca. 150.000 pages). The texts are based on the English Short Title Catalogue and are freely available to the public since 2014. It includes significant English-language printed in the United Kingdom during the 18th century. Further information on ECCO can be found here: https://textcreationpartnership.org/tcp-texts/ecco-tcp-eighteenth-century-collections-online/ The corresponding data can be retrieved here: https://www.dropbox.com/sh/inhwjphw682i2gf/AAC8NixNye8Gp0smYBTly2Y9a?dl=0 Evans Early American Imprints (Evans) TCP Evans-TCP contains about 5000 keyed and searchable published texts from the Early American Imprints online database (ca. 333.000 pages) and has been free to the public since 2014. The texts are based on the American Bibliography by Charles Evans and Roger Bristol's Supplement to Evans' American Bibliography. For the majority, the texts were published in America. For more information ,see the description of the Evans-TCP project at the University of Michigan Library: https://textcreationpartnership.org/tcp-texts/evans-tcp-evans-early-american-imprints/ The data for Evans-TCP is freely available to the public via download from the University of Michigan. It can be retrieved here: https://graphics.cs.wisc.edu/WP/vep/vep-tcp-collection/","title":"Data"},{"location":"Background/DATA/#the-data","text":"This project is centered on four datasets:","title":"The data"},{"location":"Background/DATA/#founders-online","text":"This project examined the entire corpus of 185.000 documents in the US National Archives\u2019 Founders Online database. This database brings together the bulk of the edited and collected papers of seven so-called Founding Fathers from existing print editions as well as from additional online-only sources; a total of 294 print volumes. The archive is a rich digital collection of primary source documents from the most prominent figures in early American history, including George Washington, John Adams, Thomas Jefferson, Alexander Hamilton, and James Madison. The database also includes the papers of Benjamin Franklin, John Jay, and the records of the Continental Congress. The documents are fully searchable and are available in both digital facsimile and transcribed formats. For a description of the Founders Online project and its contents, see the \u201cAbout\u201d page (https://founders.archives.gov/about) and the complete list of volumes (https://founders.archives.gov/content/volumes). The data was scraped from the Founders Online website, and can be retrieved here: https://github.com/jaytimm/founders-online-corpus. For more recently scraped data, go to https://github.com/h-gear/Explore_with_R/tree/main/data","title":"Founders online"},{"location":"Background/DATA/#letters-of-delegates-to-congress","text":"This collection of 25 volumes contains all public laws, congressional debates, and (to a limited extent) correspondence of members of Congress for the period of 1774-1789. Letters from delegates comprise most of the entries, although there are also (parts of) public papers, essays, and diaries. In total, ca. 15.000 documents are included. The data was scraped from the Library of Congress' American Memory project. Further information on this data can be found here: https://memory.loc.gov/ammem/amlaw/lwdg.html The corresponding data can be retrieved here: https://github.com/h-gear/revolution/tree/main/data/processed/delegates","title":"Letters of Delegates to Congress"},{"location":"Background/DATA/#eighteenth-century-collections-online-ecco-tcp","text":"The Eighteenth Century Collections Online (ECCO) TCP contains about 3100 keyed and searchable published texts from the Eighteenth Century Collections Online database (ca. 150.000 pages). The texts are based on the English Short Title Catalogue and are freely available to the public since 2014. It includes significant English-language printed in the United Kingdom during the 18th century. Further information on ECCO can be found here: https://textcreationpartnership.org/tcp-texts/ecco-tcp-eighteenth-century-collections-online/ The corresponding data can be retrieved here: https://www.dropbox.com/sh/inhwjphw682i2gf/AAC8NixNye8Gp0smYBTly2Y9a?dl=0","title":"Eighteenth Century Collections Online (ECCO) TCP"},{"location":"Background/DATA/#evans-early-american-imprints-evans-tcp","text":"Evans-TCP contains about 5000 keyed and searchable published texts from the Early American Imprints online database (ca. 333.000 pages) and has been free to the public since 2014. The texts are based on the American Bibliography by Charles Evans and Roger Bristol's Supplement to Evans' American Bibliography. For the majority, the texts were published in America. For more information ,see the description of the Evans-TCP project at the University of Michigan Library: https://textcreationpartnership.org/tcp-texts/evans-tcp-evans-early-american-imprints/ The data for Evans-TCP is freely available to the public via download from the University of Michigan. It can be retrieved here: https://graphics.cs.wisc.edu/WP/vep/vep-tcp-collection/","title":"Evans Early American Imprints (Evans) TCP"},{"location":"Background/METHODS/","text":"Key methods 1. Shifting concepts in time (shico) Based on Kenter et al. (2015), Martinez-Ortiz et al. (2016) developed the ShiCo tool: a system for illustrating the evolving meanings of words over time and thus mapping the conceptual change of a concept (here: liberal political ideology and republican political ideology ). This approach creates semantic spaces through the training of word2vec word embeddings across different time spans (e.g., 5 or 10 year sliding windows). First, an initial set of terms provided by the user serves as a seed. Words with high semantic similarity to this seed set are identified based on computed similarity values from word embeddings. A semantic graph is constructed using these terms, and central terms are determined with centrality measures. Subsequently, these central terms can become the seed set for the next iteration. In this step, the word lists generated in each iteration are combined to create the final word lists for user presentation. The visualization includes several complementary graphs: a stream graph and a series of network graphs. The stream graph displays color-coded streams for each term, where stream sizes indicate the term's relative importance in a given period. Importance is measured either by term count or the sum of similarities to seed terms. The network graphs for each measured time interval depict the relationships between terms during that period. For more information ,see: * Kenter, T., Wevers, M, Huijnen, P., and de Rijke, M.(2015). Ad hoc monitoring of vocabulary shifts over time. In: Proceedings of the 24th ACM international Conference on Information and Knowledge Management (CIKM\u201915) Martinez-Ortiz, C., Kenter, T., Wevers, M., Huijnen, P., Verheul, J., and van Eijnatten, J. (2016). \u201cDesign and implementation of ShiCo: visualising shifting concepts over time,\u201d in Proceedings of the 3th Histoinformatics Conference , eds M. During, A. Jatowt, A. van den Bosch, and J. Preiser-Kappeller (Krakow) https://research-software-directory.org/projects/mining-shifting-concepts-through-time-shico https://github.com/NLeSC/ShiCo link to Erik's document 2. Temporal social network analysis To conduct a temporal social network analysis on our datasets (most notably the Founding Father dataset), we took a two-step approach. First, because of the heterogeneity of the data, a semi-supervised topic modeling approach allowed to filter those letters whose central topic were on liberal and/ or republican politics. Based on the obtained homogeneous subsets of data, we then proceeded with the network analyses. 2a. Semi-supervised topic analysis We used Seeded LDA (Latent Dirichlet Allocation) to deductively identify the pre-defined political topics in the corpus of letters based on a number of central seed words derived from theory. In addition, we also allowed for unseeded topics to be present; its exact amount based on criteria such as the divergence metric, which maximizes when the chosen number of topic k is optimal. In addition, we used the keyATM package to explore if and how the prevalence of topics change over time. 2b. Path-based temporal network Pathpy is an open source python package used for network analytics for time series data as part of complex temporal graphs. Time-stamped correspondence data like we used in this project is well suited to be analyzed with pathpy. It allows to calculate paths in dynamic social networks. In the current project, a path represents a chain of letters sent between people that acounts for the order of the sending dates while at the same allowing to set a maximum time difference between dates which can be used as a cutoff (i.e., when leters are no longer assumed to be related to each other). Taken together, these tools have proven effective for identifying key themes within large bodies of correspondence letter data and then for charting temporal networks and pinpointing important actors within those networks. This project will offer the research community a powerful set of (improved) methods, tools, and data for the study of conceptual change and the transmission of ideas in historical texts and networks. For more information, see: Hackl, J. et al.(2021). Analysis and visualisation of time series data on networks with pathpy. In Companion Proceedings of the Web Conference, WWW \u201921, 530\u2013532 (Association for Computing Machinery,New York, NY, USA) Scholtes, Ingo (2017). When is a network a network? Multi-order graphical model selection in pathways and temporal networks. In: Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining , 1037\u2013 1046 Scholtes, I., Wider, N. & Garas, A. (2016). Higher-order aggregate networks in the analysis of temporal networks:path structures and centralities. The European Physical Journal B 89 , 61. Watanabe, K., & Baturo, A. (2023). Seeded Sequential LDA: A Semi-Supervised Algorithm for Topic-Specific Analysis of Sentences. Social Science Computer Review . https://doi.org/10.1177/08944393231178605 3. Animated wordcloud An animated adaptation of the static wordcloud is designed here for the letter correspondence data. Unlike the conventional word cloud, which doesn't account for temporal changes in text data, the animated version addresses this limitation. More specifically, it builds further on the results from the topic analysis by visually depicting how the words as part of a selected topic change in frequency (i.e., informing on the change in relative importance of a word as part of a topic) through time. The animated worcloud can be used both within R ( animated_wc.R ) or accessed via a shiny app ( wordcloud_app.R ). Based on the derived topics underlying the Founders Online dataset, the shiny app allows to easily select a topic of interest (e.g., \"02_Republican politics\") from a menu, after which the animated wordcloud is shown. This animated wordcloud serves as an alternative approach to the way Shico presents its findings. Simply put, whereas shico accounts for the flexible way in which a concept (e.g.,liberal politics) is denoted by showing a continously evolving wordset through time, the animated wordcloud is based on a fixed wordset derived from topic analysis, and then shows how the relative importance of each word within the fixed wordset changes over time. As such, the animated wordcloud is based on the topic-words frequency, providing a slighty different view on understanding the conceptual change of topics.","title":"Methods"},{"location":"Background/METHODS/#key-methods","text":"","title":"Key methods"},{"location":"Background/METHODS/#1-shifting-concepts-in-time-shico","text":"Based on Kenter et al. (2015), Martinez-Ortiz et al. (2016) developed the ShiCo tool: a system for illustrating the evolving meanings of words over time and thus mapping the conceptual change of a concept (here: liberal political ideology and republican political ideology ). This approach creates semantic spaces through the training of word2vec word embeddings across different time spans (e.g., 5 or 10 year sliding windows). First, an initial set of terms provided by the user serves as a seed. Words with high semantic similarity to this seed set are identified based on computed similarity values from word embeddings. A semantic graph is constructed using these terms, and central terms are determined with centrality measures. Subsequently, these central terms can become the seed set for the next iteration. In this step, the word lists generated in each iteration are combined to create the final word lists for user presentation. The visualization includes several complementary graphs: a stream graph and a series of network graphs. The stream graph displays color-coded streams for each term, where stream sizes indicate the term's relative importance in a given period. Importance is measured either by term count or the sum of similarities to seed terms. The network graphs for each measured time interval depict the relationships between terms during that period. For more information ,see: * Kenter, T., Wevers, M, Huijnen, P., and de Rijke, M.(2015). Ad hoc monitoring of vocabulary shifts over time. In: Proceedings of the 24th ACM international Conference on Information and Knowledge Management (CIKM\u201915) Martinez-Ortiz, C., Kenter, T., Wevers, M., Huijnen, P., Verheul, J., and van Eijnatten, J. (2016). \u201cDesign and implementation of ShiCo: visualising shifting concepts over time,\u201d in Proceedings of the 3th Histoinformatics Conference , eds M. During, A. Jatowt, A. van den Bosch, and J. Preiser-Kappeller (Krakow) https://research-software-directory.org/projects/mining-shifting-concepts-through-time-shico https://github.com/NLeSC/ShiCo link to Erik's document","title":"1. Shifting concepts in time (shico)"},{"location":"Background/METHODS/#2-temporal-social-network-analysis","text":"To conduct a temporal social network analysis on our datasets (most notably the Founding Father dataset), we took a two-step approach. First, because of the heterogeneity of the data, a semi-supervised topic modeling approach allowed to filter those letters whose central topic were on liberal and/ or republican politics. Based on the obtained homogeneous subsets of data, we then proceeded with the network analyses.","title":"2. Temporal social network analysis"},{"location":"Background/METHODS/#2a-semi-supervised-topic-analysis","text":"We used Seeded LDA (Latent Dirichlet Allocation) to deductively identify the pre-defined political topics in the corpus of letters based on a number of central seed words derived from theory. In addition, we also allowed for unseeded topics to be present; its exact amount based on criteria such as the divergence metric, which maximizes when the chosen number of topic k is optimal. In addition, we used the keyATM package to explore if and how the prevalence of topics change over time.","title":"2a. Semi-supervised topic analysis"},{"location":"Background/METHODS/#2b-path-based-temporal-network","text":"Pathpy is an open source python package used for network analytics for time series data as part of complex temporal graphs. Time-stamped correspondence data like we used in this project is well suited to be analyzed with pathpy. It allows to calculate paths in dynamic social networks. In the current project, a path represents a chain of letters sent between people that acounts for the order of the sending dates while at the same allowing to set a maximum time difference between dates which can be used as a cutoff (i.e., when leters are no longer assumed to be related to each other). Taken together, these tools have proven effective for identifying key themes within large bodies of correspondence letter data and then for charting temporal networks and pinpointing important actors within those networks. This project will offer the research community a powerful set of (improved) methods, tools, and data for the study of conceptual change and the transmission of ideas in historical texts and networks. For more information, see: Hackl, J. et al.(2021). Analysis and visualisation of time series data on networks with pathpy. In Companion Proceedings of the Web Conference, WWW \u201921, 530\u2013532 (Association for Computing Machinery,New York, NY, USA) Scholtes, Ingo (2017). When is a network a network? Multi-order graphical model selection in pathways and temporal networks. In: Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining , 1037\u2013 1046 Scholtes, I., Wider, N. & Garas, A. (2016). Higher-order aggregate networks in the analysis of temporal networks:path structures and centralities. The European Physical Journal B 89 , 61. Watanabe, K., & Baturo, A. (2023). Seeded Sequential LDA: A Semi-Supervised Algorithm for Topic-Specific Analysis of Sentences. Social Science Computer Review . https://doi.org/10.1177/08944393231178605","title":"2b. Path-based temporal network"},{"location":"Background/METHODS/#3-animated-wordcloud","text":"An animated adaptation of the static wordcloud is designed here for the letter correspondence data. Unlike the conventional word cloud, which doesn't account for temporal changes in text data, the animated version addresses this limitation. More specifically, it builds further on the results from the topic analysis by visually depicting how the words as part of a selected topic change in frequency (i.e., informing on the change in relative importance of a word as part of a topic) through time. The animated worcloud can be used both within R ( animated_wc.R ) or accessed via a shiny app ( wordcloud_app.R ). Based on the derived topics underlying the Founders Online dataset, the shiny app allows to easily select a topic of interest (e.g., \"02_Republican politics\") from a menu, after which the animated wordcloud is shown. This animated wordcloud serves as an alternative approach to the way Shico presents its findings. Simply put, whereas shico accounts for the flexible way in which a concept (e.g.,liberal politics) is denoted by showing a continously evolving wordset through time, the animated wordcloud is based on a fixed wordset derived from topic analysis, and then shows how the relative importance of each word within the fixed wordset changes over time. As such, the animated wordcloud is based on the topic-words frequency, providing a slighty different view on understanding the conceptual change of topics.","title":"3. Animated wordcloud"},{"location":"Developers/CONTRIBUTE/","text":"CONTRIBUTE More coming soon!","title":"CONTRIBUTE"},{"location":"Developers/GUIDELINES/","text":"CONTRIBUTING GUIDELINES More coming soon!","title":"GUIDELINES"},{"location":"Modeling/DYNTOPICANALYSIS/","text":"Dynamic topic modeling Workflow Overview This document describes the process for performing dynamic topic analysis on the Founders Online texts using the keyATM package in R. The analysis focuses on understanding how the prevalence of topics evolves over time. The workflow involves loading necessary libraries, preparing data, creating a dynamic topic model, and evaluating the results. Step-by-Step Guide 1. Script Header Description: This script performs a dynamic topic analysis on the Founders Online texts, analyzing how the prevalence of topics changes over time. It uses the keyATM package to fit a dynamic keyATM model to the cleaned text data. Purpose: - Understand the evolution of topics over time. - Provide an alternative to the shifting concepts in time (shico) approach. References: - keyATM Documentation - Seeded Topic Models 2. Libraries Load the necessary R libraries: keyATM for dynamic topic modeling. parallel for parallel processing. quanteda for text analysis. tidyverse for data manipulation. future for handling parallel computing. 3. Data Prepare the text data: Load the cleaned text data from semi_supervised_topic_modleing.R . Filter out empty texts and focus on the time period between 1750 and 1825. 4. Create keyATM Docs Transform data into a format suitable for the keyATM model: Convert the text data into a keyATM -readable format. Save the transformed documents for later use. 5. Keywords Define and visualize keywords: Load keyword definitions from an external script. Process keywords for analysis, including tokenization and lemmatization. Visualize the frequency of keywords by topic and save the results. 6. Create Decade Time Index Variable Generate a time index variable: Create a period variable that represents 10-year intervals starting from 1720. 7. Dynamic keyATM Fit the dynamic keyATM model: Set up parallel processing. Initialize and fit the dynamic keyATM model with specified settings. Save the model and its output for future reference. 8. Model Evaluation Evaluate the fitted model: Inspect topic-term associations. Diagnose model performance through log-likelihood and perplexity trends. Visualize topic proportions and keyword importance over time. 9. Theta (Document-Topic Distribution) Analyze the document-topic distribution: Preprocess and format the theta matrix for plotting. Create and save plots showing topic distributions over time and their historical context. Conclusion This workflow provides a structured approach to dynamic topic analysis, focusing on how topics evolve over time using the keyATM package. By following these steps, users can gain insights into topic trends, visualize changes, and evaluate the effectiveness of the topic modeling approach.","title":"Dynamic topic modeling"},{"location":"Modeling/DYNTOPICANALYSIS/#dynamic-topic-modeling","text":"","title":"Dynamic topic modeling"},{"location":"Modeling/DYNTOPICANALYSIS/#workflow-overview","text":"This document describes the process for performing dynamic topic analysis on the Founders Online texts using the keyATM package in R. The analysis focuses on understanding how the prevalence of topics evolves over time. The workflow involves loading necessary libraries, preparing data, creating a dynamic topic model, and evaluating the results.","title":"Workflow Overview"},{"location":"Modeling/DYNTOPICANALYSIS/#step-by-step-guide","text":"","title":"Step-by-Step Guide"},{"location":"Modeling/DYNTOPICANALYSIS/#1-script-header","text":"Description: This script performs a dynamic topic analysis on the Founders Online texts, analyzing how the prevalence of topics changes over time. It uses the keyATM package to fit a dynamic keyATM model to the cleaned text data. Purpose: - Understand the evolution of topics over time. - Provide an alternative to the shifting concepts in time (shico) approach. References: - keyATM Documentation - Seeded Topic Models","title":"1. Script Header"},{"location":"Modeling/DYNTOPICANALYSIS/#2-libraries","text":"Load the necessary R libraries: keyATM for dynamic topic modeling. parallel for parallel processing. quanteda for text analysis. tidyverse for data manipulation. future for handling parallel computing.","title":"2. Libraries"},{"location":"Modeling/DYNTOPICANALYSIS/#3-data","text":"Prepare the text data: Load the cleaned text data from semi_supervised_topic_modleing.R . Filter out empty texts and focus on the time period between 1750 and 1825.","title":"3. Data"},{"location":"Modeling/DYNTOPICANALYSIS/#4-create-keyatm-docs","text":"Transform data into a format suitable for the keyATM model: Convert the text data into a keyATM -readable format. Save the transformed documents for later use.","title":"4. Create keyATM Docs"},{"location":"Modeling/DYNTOPICANALYSIS/#5-keywords","text":"Define and visualize keywords: Load keyword definitions from an external script. Process keywords for analysis, including tokenization and lemmatization. Visualize the frequency of keywords by topic and save the results.","title":"5. Keywords"},{"location":"Modeling/DYNTOPICANALYSIS/#6-create-decade-time-index-variable","text":"Generate a time index variable: Create a period variable that represents 10-year intervals starting from 1720.","title":"6. Create Decade Time Index Variable"},{"location":"Modeling/DYNTOPICANALYSIS/#7-dynamic-keyatm","text":"Fit the dynamic keyATM model: Set up parallel processing. Initialize and fit the dynamic keyATM model with specified settings. Save the model and its output for future reference.","title":"7. Dynamic keyATM"},{"location":"Modeling/DYNTOPICANALYSIS/#8-model-evaluation","text":"Evaluate the fitted model: Inspect topic-term associations. Diagnose model performance through log-likelihood and perplexity trends. Visualize topic proportions and keyword importance over time.","title":"8. Model Evaluation"},{"location":"Modeling/DYNTOPICANALYSIS/#9-theta-document-topic-distribution","text":"Analyze the document-topic distribution: Preprocess and format the theta matrix for plotting. Create and save plots showing topic distributions over time and their historical context.","title":"9. Theta (Document-Topic Distribution)"},{"location":"Modeling/DYNTOPICANALYSIS/#conclusion","text":"This workflow provides a structured approach to dynamic topic analysis, focusing on how topics evolve over time using the keyATM package. By following these steps, users can gain insights into topic trends, visualize changes, and evaluate the effectiveness of the topic modeling approach.","title":"Conclusion"},{"location":"Modeling/GEO/","text":"Geospatial distribution of letters Overview This document outlines the workflow for mapping the geospatial distribution of letters from the Founders Online and ecco datasets. The aim is to visualize and analyze the geographic locations of letters, providing insights into their distribution across different regions. It is based on the retrieval of the place where the letter was written and its geographical coordinates. This geodata allows to find central hubs of writings. Also, with this information we can make subgroups of letters based on geographical location, and examine whether there are different ideas mentioned in the letters. For example, we can compare the letters written in the US with those in Europe, or those written in the same city but in different years. There are a lot of cities with the same name in different states or/ and in different countries. If the place of writing is missing in the letter or cannot be clearly specified, we tried to extract the place of writing from other letters written by the same person within a specified time frame (30 days) to obtain the most likely place of writing. Workflow 1. Setup and Libraries Begin by loading the necessary libraries: tidyverse : For data manipulation and visualization. sf : For handling spatial data and converting data frames into spatial objects. tidygeocoder : For geocoding addresses to obtain latitude and longitude. mapview : For interactive visualization of spatial data. tigris : For accessing US shapefiles to map geographic regions. 2. Data Preparation Load Datasets Import the founders dataset, which contains coordinates of letters. Import the ecco dataset, adjusting column names to match expectations. Address Combination For both datasets, create a combined address string that includes city, state, and country. This combined address helps in accurate geocoding by incorporating as much location detail as possible. 3. Geocoding Obtain Coordinates Use the tidygeocoder package to geocode the combined addresses. This process converts addresses into geographic coordinates (latitude and longitude). Update Coordinates If geocoding fails to provide coordinates, retain existing coordinates from the dataset where available. Ensure no information is lost during this process. Filter Valid Coordinates Remove entries with missing latitude or longitude to ensure only valid geographic points are used for analysis. Convert to Spatial Objects Convert the filtered data into sf objects, which represent spatial features with coordinates in a specified coordinate reference system (CRS). Summarize Data Group the data by city and count the number of letters per city. This step helps in understanding the distribution and density of letters in different locations. 4. Visualization Interactive World Map Use the mapview package to create an interactive map showing the distribution of letters for both datasets. Different colors are used to distinguish between datasets. US State-Level Visualization Obtain US state shapefiles using tigris and join these with the letter data to visualize the number of letters by state. This visualization helps in understanding regional patterns within the US. Spatial Clustering Apply K-means clustering to categorize the geographic points into clusters. This helps in identifying distinct geographic regions where letters are concentrated. Cluster Visualization Visualize the clusters on a map to analyze the spatial distribution of letters across different clusters. Conclusion This workflow effectively combines data preparation, geocoding, and visualization techniques to map the distribution of letters across geographic locations. By following this process, you can gain insights into the spatial patterns and concentration of letters, which can inform further analysis and research. The use of interactive maps and spatial clustering enhances the understanding of geographic distributions and reveals underlying patterns within the datasets.","title":"Geo-spatial model"},{"location":"Modeling/GEO/#geospatial-distribution-of-letters","text":"","title":"Geospatial distribution of letters"},{"location":"Modeling/GEO/#overview","text":"This document outlines the workflow for mapping the geospatial distribution of letters from the Founders Online and ecco datasets. The aim is to visualize and analyze the geographic locations of letters, providing insights into their distribution across different regions. It is based on the retrieval of the place where the letter was written and its geographical coordinates. This geodata allows to find central hubs of writings. Also, with this information we can make subgroups of letters based on geographical location, and examine whether there are different ideas mentioned in the letters. For example, we can compare the letters written in the US with those in Europe, or those written in the same city but in different years. There are a lot of cities with the same name in different states or/ and in different countries. If the place of writing is missing in the letter or cannot be clearly specified, we tried to extract the place of writing from other letters written by the same person within a specified time frame (30 days) to obtain the most likely place of writing.","title":"Overview"},{"location":"Modeling/GEO/#workflow","text":"","title":"Workflow"},{"location":"Modeling/GEO/#1-setup-and-libraries","text":"Begin by loading the necessary libraries: tidyverse : For data manipulation and visualization. sf : For handling spatial data and converting data frames into spatial objects. tidygeocoder : For geocoding addresses to obtain latitude and longitude. mapview : For interactive visualization of spatial data. tigris : For accessing US shapefiles to map geographic regions.","title":"1. Setup and Libraries"},{"location":"Modeling/GEO/#2-data-preparation","text":"","title":"2. Data Preparation"},{"location":"Modeling/GEO/#load-datasets","text":"Import the founders dataset, which contains coordinates of letters. Import the ecco dataset, adjusting column names to match expectations.","title":"Load Datasets"},{"location":"Modeling/GEO/#address-combination","text":"For both datasets, create a combined address string that includes city, state, and country. This combined address helps in accurate geocoding by incorporating as much location detail as possible.","title":"Address Combination"},{"location":"Modeling/GEO/#3-geocoding","text":"","title":"3. Geocoding"},{"location":"Modeling/GEO/#obtain-coordinates","text":"Use the tidygeocoder package to geocode the combined addresses. This process converts addresses into geographic coordinates (latitude and longitude).","title":"Obtain Coordinates"},{"location":"Modeling/GEO/#update-coordinates","text":"If geocoding fails to provide coordinates, retain existing coordinates from the dataset where available. Ensure no information is lost during this process.","title":"Update Coordinates"},{"location":"Modeling/GEO/#filter-valid-coordinates","text":"Remove entries with missing latitude or longitude to ensure only valid geographic points are used for analysis.","title":"Filter Valid Coordinates"},{"location":"Modeling/GEO/#convert-to-spatial-objects","text":"Convert the filtered data into sf objects, which represent spatial features with coordinates in a specified coordinate reference system (CRS).","title":"Convert to Spatial Objects"},{"location":"Modeling/GEO/#summarize-data","text":"Group the data by city and count the number of letters per city. This step helps in understanding the distribution and density of letters in different locations.","title":"Summarize Data"},{"location":"Modeling/GEO/#4-visualization","text":"","title":"4. Visualization"},{"location":"Modeling/GEO/#interactive-world-map","text":"Use the mapview package to create an interactive map showing the distribution of letters for both datasets. Different colors are used to distinguish between datasets.","title":"Interactive World Map"},{"location":"Modeling/GEO/#us-state-level-visualization","text":"Obtain US state shapefiles using tigris and join these with the letter data to visualize the number of letters by state. This visualization helps in understanding regional patterns within the US.","title":"US State-Level Visualization"},{"location":"Modeling/GEO/#spatial-clustering","text":"Apply K-means clustering to categorize the geographic points into clusters. This helps in identifying distinct geographic regions where letters are concentrated.","title":"Spatial Clustering"},{"location":"Modeling/GEO/#cluster-visualization","text":"Visualize the clusters on a map to analyze the spatial distribution of letters across different clusters.","title":"Cluster Visualization"},{"location":"Modeling/GEO/#conclusion","text":"This workflow effectively combines data preparation, geocoding, and visualization techniques to map the distribution of letters across geographic locations. By following this process, you can gain insights into the spatial patterns and concentration of letters, which can inform further analysis and research. The use of interactive maps and spatial clustering enhances the understanding of geographic distributions and reveals underlying patterns within the datasets.","title":"Conclusion"},{"location":"Modeling/INTRODUCTION/","text":"Introduction The code for the analyses can be found in the subfolder scripts . It is structured in parallel to the structure explained above: 4_analysis_and_modeling","title":"Introduction"},{"location":"Modeling/INTRODUCTION/#introduction","text":"The code for the analyses can be found in the subfolder scripts . It is structured in parallel to the structure explained above: 4_analysis_and_modeling","title":"Introduction"},{"location":"Modeling/MODELING/","text":"Time-respecting path-based network analysis of letter correspondences Overview The purpose of this script is to analyse time-respecting paths of a letter correspondence network of writers in the era of the American Revolution. We aim to identify key persons/ writers and patterns of communication in the transmission of political ideas through letters. We distinguish between two types of political thinking: liberal ideology and republican ideology. By employing a temporal network approach, the chronological order of the edges within the network are preserved. Unlike traditional aggregated networks that disregard the temporal information that is implicit in the letter data, this approach significantly mitigates the bias in topological network measures. Workflow The workflow for analyzing time-respecting paths in a letter correspondence network involves several systematic steps. This approach integrates temporal network analysis with betweenness centrality measures to uncover key individuals and communication patterns associated with different political ideologies. Step-by-Step Guide Load Libraries Begin by loading essential libraries for data manipulation, temporal network handling, and visualization. Key libraries include datetime for date management, pathpy for temporal network analysis, pandas for data manipulation, and igraph for network analysis. Import and Prepare Data Define File Paths: Specify the paths for data and results storage to ensure that data is correctly read and processed. Select Dataset: Choose the dataset based on the ideological focus (liberal or republican). This determines the subset of data used for analysis. Load Data: Depending on the chosen dataset, load the relevant CSV files containing the correspondence links or letter information. If conducting SHICO analysis, use specific link files; for topic-based analysis, filter data based on the topic. Clean Data: Convert sending dates into time differences to facilitate temporal analysis. Rename columns for consistency and remove unnecessary columns. Prepare and save the cleaned data for temporal network analysis. Compute Time-Respecting Paths Create Temporal Network: Read the cleaned data to construct a temporal network. This network preserves the chronological order of correspondence, which is crucial for analyzing time-respecting paths. Extract Paths: Compute paths within the temporal network for various time intervals (delta_t). Save these paths to files for further analysis. Compute Betweenness Centrality Betweenness for Time-Respecting Paths: Calculate the betweenness centrality for the extracted time-respecting paths. This measure identifies key nodes (writers) that act as intermediaries in the network. Betweenness for Aggregated Network: Construct an aggregated network and compute betweenness centrality for this network to compare with time-respecting paths. This step involves building a weighted graph and calculating centrality metrics. Robustness Check: Betweenness for Different delta_t Compute Betweenness for Various delta_t: Analyze the betweenness centrality for different time intervals to assess the consistency of key individuals' importance across different temporal scales. Aggregate the results and save them for comparison. Visualize Betweenness Scores Plot Betweenness Scores: Create visualizations to compare betweenness centrality scores between time-respecting paths and the aggregated network. This helps in understanding the relative importance of nodes. Bump Chart for Betweenness Across delta_t: Use a bump chart to visualize how the ranks of writers change with different time intervals, providing insights into the stability of key nodes' roles over time. Robustness Check: Path Length Frequencies Compute Path Length Frequencies: Analyze the distribution of path lengths for different delta_t values to understand the frequency of various path lengths in the network. This helps in evaluating the robustness of the path analysis. Conclusion This workflow offers a comprehensive approach to analyzing historical letter correspondence networks by focusing on time-respecting paths and centrality measures. By preserving the chronological order of correspondence, the analysis mitigates biases found in traditional network measures. The combination of temporal network analysis and betweenness centrality provides valuable insights into key individuals and their roles within different ideological contexts. The robustness checks and visualizations further enhance the understanding of the network's structure and the stability of central nodes over time. This methodology can be instrumental in uncovering historical communication patterns and understanding the influence of individuals in the context of ideological movements.","title":"Time-respecting path-based network analysis"},{"location":"Modeling/MODELING/#time-respecting-path-based-network-analysis-of-letter-correspondences","text":"","title":"Time-respecting path-based network analysis of letter correspondences"},{"location":"Modeling/MODELING/#overview","text":"The purpose of this script is to analyse time-respecting paths of a letter correspondence network of writers in the era of the American Revolution. We aim to identify key persons/ writers and patterns of communication in the transmission of political ideas through letters. We distinguish between two types of political thinking: liberal ideology and republican ideology. By employing a temporal network approach, the chronological order of the edges within the network are preserved. Unlike traditional aggregated networks that disregard the temporal information that is implicit in the letter data, this approach significantly mitigates the bias in topological network measures.","title":"Overview"},{"location":"Modeling/MODELING/#workflow","text":"The workflow for analyzing time-respecting paths in a letter correspondence network involves several systematic steps. This approach integrates temporal network analysis with betweenness centrality measures to uncover key individuals and communication patterns associated with different political ideologies.","title":"Workflow"},{"location":"Modeling/MODELING/#step-by-step-guide","text":"","title":"Step-by-Step Guide"},{"location":"Modeling/MODELING/#load-libraries","text":"Begin by loading essential libraries for data manipulation, temporal network handling, and visualization. Key libraries include datetime for date management, pathpy for temporal network analysis, pandas for data manipulation, and igraph for network analysis.","title":"Load Libraries"},{"location":"Modeling/MODELING/#import-and-prepare-data","text":"Define File Paths: Specify the paths for data and results storage to ensure that data is correctly read and processed. Select Dataset: Choose the dataset based on the ideological focus (liberal or republican). This determines the subset of data used for analysis. Load Data: Depending on the chosen dataset, load the relevant CSV files containing the correspondence links or letter information. If conducting SHICO analysis, use specific link files; for topic-based analysis, filter data based on the topic. Clean Data: Convert sending dates into time differences to facilitate temporal analysis. Rename columns for consistency and remove unnecessary columns. Prepare and save the cleaned data for temporal network analysis.","title":"Import and Prepare Data"},{"location":"Modeling/MODELING/#compute-time-respecting-paths","text":"Create Temporal Network: Read the cleaned data to construct a temporal network. This network preserves the chronological order of correspondence, which is crucial for analyzing time-respecting paths. Extract Paths: Compute paths within the temporal network for various time intervals (delta_t). Save these paths to files for further analysis.","title":"Compute Time-Respecting Paths"},{"location":"Modeling/MODELING/#compute-betweenness-centrality","text":"Betweenness for Time-Respecting Paths: Calculate the betweenness centrality for the extracted time-respecting paths. This measure identifies key nodes (writers) that act as intermediaries in the network. Betweenness for Aggregated Network: Construct an aggregated network and compute betweenness centrality for this network to compare with time-respecting paths. This step involves building a weighted graph and calculating centrality metrics.","title":"Compute Betweenness Centrality"},{"location":"Modeling/MODELING/#robustness-check-betweenness-for-different-delta_t","text":"Compute Betweenness for Various delta_t: Analyze the betweenness centrality for different time intervals to assess the consistency of key individuals' importance across different temporal scales. Aggregate the results and save them for comparison.","title":"Robustness Check: Betweenness for Different delta_t"},{"location":"Modeling/MODELING/#visualize-betweenness-scores","text":"Plot Betweenness Scores: Create visualizations to compare betweenness centrality scores between time-respecting paths and the aggregated network. This helps in understanding the relative importance of nodes. Bump Chart for Betweenness Across delta_t: Use a bump chart to visualize how the ranks of writers change with different time intervals, providing insights into the stability of key nodes' roles over time.","title":"Visualize Betweenness Scores"},{"location":"Modeling/MODELING/#robustness-check-path-length-frequencies","text":"Compute Path Length Frequencies: Analyze the distribution of path lengths for different delta_t values to understand the frequency of various path lengths in the network. This helps in evaluating the robustness of the path analysis.","title":"Robustness Check: Path Length Frequencies"},{"location":"Modeling/MODELING/#conclusion","text":"This workflow offers a comprehensive approach to analyzing historical letter correspondence networks by focusing on time-respecting paths and centrality measures. By preserving the chronological order of correspondence, the analysis mitigates biases found in traditional network measures. The combination of temporal network analysis and betweenness centrality provides valuable insights into key individuals and their roles within different ideological contexts. The robustness checks and visualizations further enhance the understanding of the network's structure and the stability of central nodes over time. This methodology can be instrumental in uncovering historical communication patterns and understanding the influence of individuals in the context of ideological movements.","title":"Conclusion"},{"location":"Modeling/NETWORK/","text":"Network analysis and community detection Workflow Overview This document outlines the steps for performing network analysis, community detection, and calculating centralities using R. The analysis involves several key stages, including data preparation, network construction, community detection, dynamic network analysis, and centrality calculations. Step-by-Step Guide 1. Libraries Load the necessary R libraries for network analysis and visualization: - visNetwork for interactive network visualizations. - tidyverse for data manipulation and cleaning. - ndtv for dynamic network visualizations. - tsna for time series network analysis. - sna for network analysis metrics. - networkDynamic for handling dynamic networks. - igraph for community detection and additional network analysis. 2. Read Data Import and prepare the data: - Letters Data : Load a CSV file containing information about letters and their associated topics. - Preprocessed Data : Load an RDS file containing preprocessed information about the letters. Ensure the data types are correctly set for subsequent analysis. 3. Create Edge List Generate an edge list for the network: - Filter and select relevant data based on a specific topic of interest. - Compute the weight of interactions (i.e., number of letters exchanged) between sender and receiver. - Prepare the edge list by adding attributes such as onset and terminus years, edge ID, and undirected weight (if applicable). 4. Create Node List Construct a node list: - Identify unique nodes (authors and recipients) and their attributes. - Include distinguishing features like specific colors for prominent figures (e.g., founding fathers). 5. Aggregated Network Build and summarize the static network: - Create a network object using the edge list and node list. - Plot the network to visualize relationships, using attributes such as node color and edge width. 6. Community Detection Detect communities within the network: - Convert the network into an igraph object. - Apply the Louvain algorithm for community detection to identify clusters within the network. - Visualize the network with community-based color coding and node sizes proportional to centrality measures. 7. Dynamic Network Analysis Analyze the network dynamics over time: - Create a dynamic network object using edge and vertex spells. - Reconcile vertex activity with edge activity to ensure correct temporal alignment. - Visualize the network dynamics using timeline plots and network slices. 8. Centrality Calculations Calculate and visualize centralities: - Analyze graph-level statistics such as density, reciprocity, and dyad census over time. - Compute and plot node-level centralities, including degree and other metrics. 9. Animation Create animations to visualize changes in the network over time: - Compute and render an animated version of the network to reveal structural patterns and transitions. Conclusion This workflow provides a comprehensive approach to network analysis, from data preparation to dynamic analysis and visualization. By following these steps, users can gain insights into community structures, network dynamics, and centrality measures, allowing for a deeper understanding of the network's evolution and key actors.","title":"Network modeling"},{"location":"Modeling/NETWORK/#network-analysis-and-community-detection","text":"","title":"Network analysis and community detection"},{"location":"Modeling/NETWORK/#workflow-overview","text":"This document outlines the steps for performing network analysis, community detection, and calculating centralities using R. The analysis involves several key stages, including data preparation, network construction, community detection, dynamic network analysis, and centrality calculations.","title":"Workflow Overview"},{"location":"Modeling/NETWORK/#step-by-step-guide","text":"","title":"Step-by-Step Guide"},{"location":"Modeling/NETWORK/#1-libraries","text":"Load the necessary R libraries for network analysis and visualization: - visNetwork for interactive network visualizations. - tidyverse for data manipulation and cleaning. - ndtv for dynamic network visualizations. - tsna for time series network analysis. - sna for network analysis metrics. - networkDynamic for handling dynamic networks. - igraph for community detection and additional network analysis.","title":"1. Libraries"},{"location":"Modeling/NETWORK/#2-read-data","text":"Import and prepare the data: - Letters Data : Load a CSV file containing information about letters and their associated topics. - Preprocessed Data : Load an RDS file containing preprocessed information about the letters. Ensure the data types are correctly set for subsequent analysis.","title":"2. Read Data"},{"location":"Modeling/NETWORK/#3-create-edge-list","text":"Generate an edge list for the network: - Filter and select relevant data based on a specific topic of interest. - Compute the weight of interactions (i.e., number of letters exchanged) between sender and receiver. - Prepare the edge list by adding attributes such as onset and terminus years, edge ID, and undirected weight (if applicable).","title":"3. Create Edge List"},{"location":"Modeling/NETWORK/#4-create-node-list","text":"Construct a node list: - Identify unique nodes (authors and recipients) and their attributes. - Include distinguishing features like specific colors for prominent figures (e.g., founding fathers).","title":"4. Create Node List"},{"location":"Modeling/NETWORK/#5-aggregated-network","text":"Build and summarize the static network: - Create a network object using the edge list and node list. - Plot the network to visualize relationships, using attributes such as node color and edge width.","title":"5. Aggregated Network"},{"location":"Modeling/NETWORK/#6-community-detection","text":"Detect communities within the network: - Convert the network into an igraph object. - Apply the Louvain algorithm for community detection to identify clusters within the network. - Visualize the network with community-based color coding and node sizes proportional to centrality measures.","title":"6. Community Detection"},{"location":"Modeling/NETWORK/#7-dynamic-network-analysis","text":"Analyze the network dynamics over time: - Create a dynamic network object using edge and vertex spells. - Reconcile vertex activity with edge activity to ensure correct temporal alignment. - Visualize the network dynamics using timeline plots and network slices.","title":"7. Dynamic Network Analysis"},{"location":"Modeling/NETWORK/#8-centrality-calculations","text":"Calculate and visualize centralities: - Analyze graph-level statistics such as density, reciprocity, and dyad census over time. - Compute and plot node-level centralities, including degree and other metrics.","title":"8. Centrality Calculations"},{"location":"Modeling/NETWORK/#9-animation","text":"Create animations to visualize changes in the network over time: - Compute and render an animated version of the network to reveal structural patterns and transitions.","title":"9. Animation"},{"location":"Modeling/NETWORK/#conclusion","text":"This workflow provides a comprehensive approach to network analysis, from data preparation to dynamic analysis and visualization. By following these steps, users can gain insights into community structures, network dynamics, and centrality measures, allowing for a deeper understanding of the network's evolution and key actors.","title":"Conclusion"},{"location":"Modeling/SENTIMENT/","text":"Sentiment analysis by topic per year Description This script performs sentiment analysis on historical texts, categorized by topic and year. It evaluates the sentiment associated with each topic over time and visualizes sentiment trends through various plots. By integrating historical events and topic analysis results, the script provides insights into how sentiment evolves within specific topics across different periods. This analysis supports a deeper understanding of the thematic shifts and emotional tone in historical letters. Purpose Analyze Sentiment by Topic: To assess the sentiment associated with different topics over time. Visualize Sentiment Trends: To create visual representations of sentiment changes, including ridge plots and time series. Compare Sentiments: To generate word clouds contrasting positive and negative sentiments for additional insights. Input Dataset Historical Events: A CSV file containing significant historical events with start and end dates. Topic-Analytical Results: A CSV file with preprocessed texts including topic information and clean text. Output Figures: TIFF files containing visualizations of sentiment trends, including sentiment over time for specific topics, overall topic sentiment trends, and word clouds. Workflow Import Libraries Load necessary R libraries for text processing, sentiment analysis, and visualization. This includes packages like sentopics , tidytext , ggridges , tidyverse , reshape2 , wordcloud , zoo , and lubridate . Read Data Historical Events: Load historical events data and parse dates. Topic-Analytical Results: Load text data with topic information and preprocess dates for analysis. Prepare Data Text Data: Ensure each word is in a separate row and format the dataset for sentiment analysis by year and topic. Historical Time Periods: Define key historical periods with start and end dates for contextual analysis. Calculate Sentiment Per Topic Per Year Sentiment Analysis: Join text data with sentiment lexicons (AFINN) to calculate average sentiment scores by topic and year. Visualize Sentiment for Individual Topics Plot Sentiment for Specific Topics: Generate and save TIFF files showing sentiment trends over time for specific topics, e.g., \"Republican politics\" and \"Liberal politics.\" Visualize Sentiment Development for All Topics Ridge Graph: Create a ridge plot displaying sentiment development across all topics over time. Use a dynamic color palette to represent different topics. Generate Word Clouds Positive vs. Negative Sentiments: Create word clouds to visualize the most frequent words associated with positive and negative sentiments, providing insights into the emotional tone of the texts. Save Results Save the generated plots and word clouds as TIFF files for further analysis and reporting. Conclusion This workflow provides a detailed sentiment analysis of historical texts, categorized by topic and year. By leveraging various visualizations, including time series plots, ridge graphs, and word clouds, the analysis offers a comprehensive view of how sentiment evolves within different themes over time. This approach enhances the understanding of historical texts, revealing shifts in emotional tone and thematic focus, and supports both qualitative and quantitative research efforts.","title":"Sentiment analysis"},{"location":"Modeling/SENTIMENT/#sentiment-analysis-by-topic-per-year","text":"","title":"Sentiment analysis by topic per year"},{"location":"Modeling/SENTIMENT/#description","text":"This script performs sentiment analysis on historical texts, categorized by topic and year. It evaluates the sentiment associated with each topic over time and visualizes sentiment trends through various plots. By integrating historical events and topic analysis results, the script provides insights into how sentiment evolves within specific topics across different periods. This analysis supports a deeper understanding of the thematic shifts and emotional tone in historical letters.","title":"Description"},{"location":"Modeling/SENTIMENT/#purpose","text":"Analyze Sentiment by Topic: To assess the sentiment associated with different topics over time. Visualize Sentiment Trends: To create visual representations of sentiment changes, including ridge plots and time series. Compare Sentiments: To generate word clouds contrasting positive and negative sentiments for additional insights.","title":"Purpose"},{"location":"Modeling/SENTIMENT/#input-dataset","text":"Historical Events: A CSV file containing significant historical events with start and end dates. Topic-Analytical Results: A CSV file with preprocessed texts including topic information and clean text.","title":"Input Dataset"},{"location":"Modeling/SENTIMENT/#output","text":"Figures: TIFF files containing visualizations of sentiment trends, including sentiment over time for specific topics, overall topic sentiment trends, and word clouds.","title":"Output"},{"location":"Modeling/SENTIMENT/#workflow","text":"Import Libraries Load necessary R libraries for text processing, sentiment analysis, and visualization. This includes packages like sentopics , tidytext , ggridges , tidyverse , reshape2 , wordcloud , zoo , and lubridate . Read Data Historical Events: Load historical events data and parse dates. Topic-Analytical Results: Load text data with topic information and preprocess dates for analysis. Prepare Data Text Data: Ensure each word is in a separate row and format the dataset for sentiment analysis by year and topic. Historical Time Periods: Define key historical periods with start and end dates for contextual analysis. Calculate Sentiment Per Topic Per Year Sentiment Analysis: Join text data with sentiment lexicons (AFINN) to calculate average sentiment scores by topic and year. Visualize Sentiment for Individual Topics Plot Sentiment for Specific Topics: Generate and save TIFF files showing sentiment trends over time for specific topics, e.g., \"Republican politics\" and \"Liberal politics.\" Visualize Sentiment Development for All Topics Ridge Graph: Create a ridge plot displaying sentiment development across all topics over time. Use a dynamic color palette to represent different topics. Generate Word Clouds Positive vs. Negative Sentiments: Create word clouds to visualize the most frequent words associated with positive and negative sentiments, providing insights into the emotional tone of the texts. Save Results Save the generated plots and word clouds as TIFF files for further analysis and reporting.","title":"Workflow"},{"location":"Modeling/SENTIMENT/#conclusion","text":"This workflow provides a detailed sentiment analysis of historical texts, categorized by topic and year. By leveraging various visualizations, including time series plots, ridge graphs, and word clouds, the analysis offers a comprehensive view of how sentiment evolves within different themes over time. This approach enhances the understanding of historical texts, revealing shifts in emotional tone and thematic focus, and supports both qualitative and quantitative research efforts.","title":"Conclusion"},{"location":"Modeling/TOPICANALYSIS/","text":"Semi-supervised topic modeling Description This script performs a comprehensive topic analysis on the Founders Online texts. It categorizes letters based on their dominant topics, creating a framework to analyze subsets of letters according to specific themes. This method complements the results from the Shico tool, which selects homogeneous letters based on word occurrences related to particular concepts. The combined approach of topic analysis and Shico's results provides a robust method to validate findings and refine topic-based subsets. Ultimately, this processed data supports the creation of animated word clouds and network analyses for deeper insights into the topics of interest. Purpose Assess Topic Diversity: To understand the range of topics present in the letters. Create Homogeneous Subsets: To generate subsets of letters categorized by different political ideologies as identified by the lead applicant. Validate Shico Results: To cross-check the results from Shico with the topic analysis to ensure consistency and accuracy. Generate Input for Visualization: To prepare datasets suitable for creating animated word clouds, enhancing topic visualization. Input Dataset Letters: The preprocessed dataset from Founders Online, including letter IDs and text. Output Data Frame: A data frame that includes an additional column indicating the dominant topic for each letter and topic probabilities for all topics. Workflow Import Libraries Begin by loading the necessary libraries for text processing and topic modeling. This includes tools for parallel computation and specialized packages such as quanteda , seededlda , and furrr . Read and Prepare Data Load the preprocessed dataset of letters. Ensure the letters are sorted by sending date, and verify that each letter is correctly formatted for subsequent analysis. Text Preprocessing Part-of-Speech Tagging: Segment the text into chunks and extract nouns. This process may be divided into multiple files to handle large datasets efficiently. Tokenization: Convert text to lowercase, remove abbreviations and stopwords, and create tokens. Identify and integrate multi-word expressions into tokens. Lemmatization: Use a predefined lemma table to reduce words to their base forms, ensuring consistency in the text analysis. Combine and Clean Data Combine the original and cleaned versions of the letters. Create a document-feature matrix (DFM) from the lemmatized tokens, and refine it by removing infrequent and overly common terms. Topic Modeling Initial Topic Count: Experiment with different numbers of topics using measures like Kullback-Leibler divergence to find the optimal count. Model Testing: Utilize the seededlda approach to guide topic formation with specific keywords. Test multiple models with varying numbers of topics to identify the most meaningful configuration. Evaluate Models Assess the candidate models for coherence and exclusivity. These evaluations help determine the quality and distinctiveness of the topics, guiding the selection of the most representative models. Save Results Save the processed data and model results in RDS format for future use. This includes the final document-feature matrix, topic models, and any intermediate outputs. Topic Probabilities per Letter Calculate the topic probabilities for each letter. This involves extracting the topic probabilities from the model results and transforming them into a format suitable for further analysis. Most-likely Topic for Each Letter Determine the dominant topic for each letter based on the highest probability and add this information to the original texts dataframe. This allows for more focused analyses and visualizations. Distribution and Visualization Distribution of Letters per Main Topic: Analyze and visualize the distribution of letters across different topics. Generate a bar plot to show the proportion of letters associated with each topic. Most Prevalent Topics: Identify and visualize the most prevalent topics overall. Create a horizontal bar plot to display the prevalence of each topic. Topic Prevalence by Year: Examine how topic prevalence changes over time. Calculate the mean topic prevalence by year and visualize temporal trends in topic distribution. Visualize Word Probabilities per Topic Visualize the top words associated with each topic. Plot the top terms for each topic to understand the key themes and their significance. LDAvis Utilize LDAvis for interactive visualization of the LDA topic model. This tool helps in exploring topic-term relationships and provides a graphical representation of the topics and their associated terms. Save Data for Further Analysis Prepare and save dataframes for additional analyses, such as animated word clouds and network analyses. This includes merging topic probabilities with text data and creating datasets for detailed exploration. Conclusion This workflow efficiently manages large text datasets by integrating text preprocessing, topic modeling, and validation techniques. The combination of topic analysis with Shico's results offers robust insights into letter themes and supports further exploration through visualizations such as animated word clouds and network analyses. This comprehensive approach facilitates a deeper understanding of the political ideologies and themes in historical letters, aiding both qualitative and quantitative research.","title":"Semi-supervised topic modeling"},{"location":"Modeling/TOPICANALYSIS/#semi-supervised-topic-modeling","text":"","title":"Semi-supervised topic modeling"},{"location":"Modeling/TOPICANALYSIS/#description","text":"This script performs a comprehensive topic analysis on the Founders Online texts. It categorizes letters based on their dominant topics, creating a framework to analyze subsets of letters according to specific themes. This method complements the results from the Shico tool, which selects homogeneous letters based on word occurrences related to particular concepts. The combined approach of topic analysis and Shico's results provides a robust method to validate findings and refine topic-based subsets. Ultimately, this processed data supports the creation of animated word clouds and network analyses for deeper insights into the topics of interest.","title":"Description"},{"location":"Modeling/TOPICANALYSIS/#purpose","text":"Assess Topic Diversity: To understand the range of topics present in the letters. Create Homogeneous Subsets: To generate subsets of letters categorized by different political ideologies as identified by the lead applicant. Validate Shico Results: To cross-check the results from Shico with the topic analysis to ensure consistency and accuracy. Generate Input for Visualization: To prepare datasets suitable for creating animated word clouds, enhancing topic visualization.","title":"Purpose"},{"location":"Modeling/TOPICANALYSIS/#input-dataset","text":"Letters: The preprocessed dataset from Founders Online, including letter IDs and text.","title":"Input Dataset"},{"location":"Modeling/TOPICANALYSIS/#output","text":"Data Frame: A data frame that includes an additional column indicating the dominant topic for each letter and topic probabilities for all topics.","title":"Output"},{"location":"Modeling/TOPICANALYSIS/#workflow","text":"Import Libraries Begin by loading the necessary libraries for text processing and topic modeling. This includes tools for parallel computation and specialized packages such as quanteda , seededlda , and furrr . Read and Prepare Data Load the preprocessed dataset of letters. Ensure the letters are sorted by sending date, and verify that each letter is correctly formatted for subsequent analysis. Text Preprocessing Part-of-Speech Tagging: Segment the text into chunks and extract nouns. This process may be divided into multiple files to handle large datasets efficiently. Tokenization: Convert text to lowercase, remove abbreviations and stopwords, and create tokens. Identify and integrate multi-word expressions into tokens. Lemmatization: Use a predefined lemma table to reduce words to their base forms, ensuring consistency in the text analysis. Combine and Clean Data Combine the original and cleaned versions of the letters. Create a document-feature matrix (DFM) from the lemmatized tokens, and refine it by removing infrequent and overly common terms. Topic Modeling Initial Topic Count: Experiment with different numbers of topics using measures like Kullback-Leibler divergence to find the optimal count. Model Testing: Utilize the seededlda approach to guide topic formation with specific keywords. Test multiple models with varying numbers of topics to identify the most meaningful configuration. Evaluate Models Assess the candidate models for coherence and exclusivity. These evaluations help determine the quality and distinctiveness of the topics, guiding the selection of the most representative models. Save Results Save the processed data and model results in RDS format for future use. This includes the final document-feature matrix, topic models, and any intermediate outputs. Topic Probabilities per Letter Calculate the topic probabilities for each letter. This involves extracting the topic probabilities from the model results and transforming them into a format suitable for further analysis. Most-likely Topic for Each Letter Determine the dominant topic for each letter based on the highest probability and add this information to the original texts dataframe. This allows for more focused analyses and visualizations. Distribution and Visualization Distribution of Letters per Main Topic: Analyze and visualize the distribution of letters across different topics. Generate a bar plot to show the proportion of letters associated with each topic. Most Prevalent Topics: Identify and visualize the most prevalent topics overall. Create a horizontal bar plot to display the prevalence of each topic. Topic Prevalence by Year: Examine how topic prevalence changes over time. Calculate the mean topic prevalence by year and visualize temporal trends in topic distribution. Visualize Word Probabilities per Topic Visualize the top words associated with each topic. Plot the top terms for each topic to understand the key themes and their significance. LDAvis Utilize LDAvis for interactive visualization of the LDA topic model. This tool helps in exploring topic-term relationships and provides a graphical representation of the topics and their associated terms. Save Data for Further Analysis Prepare and save dataframes for additional analyses, such as animated word clouds and network analyses. This includes merging topic probabilities with text data and creating datasets for detailed exploration.","title":"Workflow"},{"location":"Modeling/TOPICANALYSIS/#conclusion","text":"This workflow efficiently manages large text datasets by integrating text preprocessing, topic modeling, and validation techniques. The combination of topic analysis with Shico's results offers robust insights into letter themes and supports further exploration through visualizations such as animated word clouds and network analyses. This comprehensive approach facilitates a deeper understanding of the political ideologies and themes in historical letters, aiding both qualitative and quantitative research.","title":"Conclusion"},{"location":"Started/INSTALLATION/","text":"Installation These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. To install hgear from this GitHub repository, do: git clone git@github.com:https://github.com/h-gear/revolution.git Installation Here, we give a step by step instructions that tell you how to get you up and running. The code is written mostly in R and parts in Python. For R , RStudio is recommended, while for Python, we recommend anaconda . Setting up the Virtual Environment for R To ensure a consistent and reproducible environment, this project uses the venv package in R. Follow these steps to set up the virtual environment and install the necessary packages: Navigate to the Project Directory: cd revolution Run the Setup Script: Execute the provided setup script (setup.bat) to automatically create and activate the virtual environment, and install the necessary packages: setup.bat This script will install the renv package, create and activate the virtual environment, and install the required R packages specified in renv.lock. By running the provided setup script, Windows users can easily set up the required environment without worrying about manually installing packages. This approach streamlines the process and ensures that users have the correct dependencies in place. Note: Ensure that the script is executed with administrative privileges. If prompted, right-click on setup.bat and choose \"Run as administrator.\" Run Your Code: After the setup script completes, you are ready to run the code within the virtual environment. Execute your R scripts or run your Rmarkdown documents as usual. Deactivate the Virtual Environment: When you are done, deactivate the virtual environment: venv::deactivate() Python: Setting up a Virtual Environment We recommend installing hgear in a new virtual environment to avoid dependency conflicts. Run the following commands to create a virtual environment: For Windows : python -m venv ./.venv For Unix or MacOS : python3 -m venv ./.venv Activate the Virtual Environment: For Windows : .venv\\Scripts\\activate.bat For Unix or MacOS : source .venv/bin/activate Install Dependencies: Once the virtual environment is created and activated you can install the dependencies by running: pip install -r requirements.txt At this stage, you should be able to run the scripts. Deactivate the Virtual Environment: When you are done, deactivate the virtual environment: deactivate","title":"Installation"},{"location":"Started/INSTALLATION/#installation","text":"These instructions will get you a copy of the project up and running on your local machine for development and testing purposes. To install hgear from this GitHub repository, do: git clone git@github.com:https://github.com/h-gear/revolution.git","title":"Installation"},{"location":"Started/INSTALLATION/#installation_1","text":"Here, we give a step by step instructions that tell you how to get you up and running. The code is written mostly in R and parts in Python. For R , RStudio is recommended, while for Python, we recommend anaconda .","title":"Installation"},{"location":"Started/INSTALLATION/#setting-up-the-virtual-environment-for-r","text":"To ensure a consistent and reproducible environment, this project uses the venv package in R. Follow these steps to set up the virtual environment and install the necessary packages: Navigate to the Project Directory: cd revolution Run the Setup Script: Execute the provided setup script (setup.bat) to automatically create and activate the virtual environment, and install the necessary packages: setup.bat This script will install the renv package, create and activate the virtual environment, and install the required R packages specified in renv.lock. By running the provided setup script, Windows users can easily set up the required environment without worrying about manually installing packages. This approach streamlines the process and ensures that users have the correct dependencies in place. Note: Ensure that the script is executed with administrative privileges. If prompted, right-click on setup.bat and choose \"Run as administrator.\" Run Your Code: After the setup script completes, you are ready to run the code within the virtual environment. Execute your R scripts or run your Rmarkdown documents as usual. Deactivate the Virtual Environment: When you are done, deactivate the virtual environment: venv::deactivate()","title":"Setting up the Virtual Environment for R"},{"location":"Started/INSTALLATION/#python-setting-up-a-virtual-environment","text":"We recommend installing hgear in a new virtual environment to avoid dependency conflicts. Run the following commands to create a virtual environment: For Windows : python -m venv ./.venv For Unix or MacOS : python3 -m venv ./.venv Activate the Virtual Environment: For Windows : .venv\\Scripts\\activate.bat For Unix or MacOS : source .venv/bin/activate Install Dependencies: Once the virtual environment is created and activated you can install the dependencies by running: pip install -r requirements.txt At this stage, you should be able to run the scripts. Deactivate the Virtual Environment: When you are done, deactivate the virtual environment: deactivate","title":"Python: Setting up a Virtual Environment"},{"location":"Started/ISSUES/","text":"KNOWN ISSUES Known issues: runtime issues that users might experience (with workarounds if available) and misconceptions about how SN works or what it is supposed to do. The following is a list of known issues as part of working with the Orange3 StoryNavigator Add-on: 1 \u2026.. 2 \u2026\u2026 3 \u2026\u2026 This list of issues can be further updated in the future.","title":"ISSUES"}]}